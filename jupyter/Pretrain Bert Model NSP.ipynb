{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cde9f7-cf94-4c69-8b34-a5f72c58024e",
   "metadata": {},
   "source": [
    "# Pretraining (NSP : NEXT SENTENCE PREDICTION)\n",
    "\n",
    "## PROCESS OVERVIEW\n",
    "\n",
    "\n",
    "\n",
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   NOW   |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |\n",
    "\n",
    "---\n",
    "\n",
    "TUJUAN :\n",
    "\n",
    "- Bukan cuma paham hubungan kata di kalimat, tapi juga hubungan antar kalimat\n",
    "- Apakah kalimat kedua nyambung?\n",
    "\n",
    "HOW? :\n",
    "\n",
    "- Kalimat berhubungan -> Label NSP = IsNext\n",
    "- Kalimat tidak berhubungan -> Label NSP = NotNext\n",
    "\n",
    "- [CLS] Kalimat 1 [SEP] Kalimat 2 [SEP]\n",
    "- Token [CLS] itu direpresentasikan jadi 1 vektor lewat encoder.\n",
    "- Vektor [CLS] ini dipakai buat prediksi IsNext atau NotNext lewat 1 layer klasifikasi.\n",
    "\n",
    "STEPS :\n",
    "1. Input :\n",
    "- c = ['kucing bermain di taman']\n",
    "- t = ['kucing', 'bermain', 'di', 'taman']\n",
    "2. Special Token :\n",
    "- ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]']\n",
    "3. Ambil Output vektor [CLS]\n",
    "4. Linear Layer untuk klasifikasi IsNext/NotNext\n",
    "5. Loss Function\n",
    "6. Optimizer\n",
    "7. Training\n",
    "\n",
    "| Misi                              | Tujuan                         | Gampangnya                                     |\n",
    "| :-------------------------------- | :----------------------------- | :--------------------------------------------- |\n",
    "| 1. Masked Language Model (MLM)    | Belajar isi kata yang hilang   | Tebak kata yang ketutupan                      |\n",
    "| 2. Next Sentence Prediction (NSP) | Belajar hubungan antar kalimat | Tebak apakah kalimat kedua nyambung atau ngaco |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45eea8-0591-4067-b90d-c1141e3d5614",
   "metadata": {},
   "source": [
    "# NSP : NEXT SENTENCE PREDICTION\n",
    "\n",
    "![](https://amitness.com/posts/images/bert-nsp.png)\n",
    "\n",
    "\n",
    "## INTUITION\n",
    "\n",
    "- cls merepresentasikan kalimat dan hubungan antar kalimat\n",
    "\n",
    "\n",
    "\n",
    "## PROCESS\n",
    "\n",
    "1. Input :\n",
    "\n",
    "- s1 = ['kucing bermain di taman']\n",
    "\n",
    "- s2 = ['kucing makan nasi']\n",
    "\n",
    "\n",
    "2. Special Token :\n",
    "\n",
    "- ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]', 'kucing', 'makan', 'nasi', '[SEP]']\n",
    "\n",
    "\n",
    "3. Pretrain Model with this Approach :\n",
    "\n",
    "- Input : ['[CLS]', 'kucing', 'bermain', 'di', 'taman', '[SEP]', 'kucing', 'makan', 'nasi', '[SEP']\n",
    "\n",
    "- Embedding (token embedding + positional embedding),\n",
    "  \n",
    "- Stack Encoder stack (MHA âž” AddNorm âž” FFN âž” AddNorm),\n",
    "\n",
    "- inti : train cls untuk memprediksi apakah kalimat kedua merupakan kalimat berikutnya dari kalimat pertama atau tidak\n",
    "\n",
    "## PSEUDOCODE\n",
    "\n",
    "    # pretraining bert for mlm\n",
    "    initialize bert model with random weight\n",
    "\n",
    "    def apply mask (tokens):\n",
    "        for i in range (len token):\n",
    "            if random < 0.15:\n",
    "                if random < 0.8:\n",
    "                    tokens[i] = [mask]\n",
    "                elif random < 0.9:\n",
    "                    token[i] = random_token()\n",
    "                else:\n",
    "                    token[i] = token[i]\n",
    "                lebel[i] = original token\n",
    "            else:\n",
    "                label[i] = [ignore]\n",
    "\n",
    "        return tokens, label\n",
    "    \n",
    "    for each epoch:\n",
    "        for each batch in training data :\n",
    "        # 1. tokenize\n",
    "        input token = tokenize(batch)\n",
    "\n",
    "        # 2. masking\n",
    "        mask input, label = apply mask (input token)\n",
    "\n",
    "        # 3. feed forward bert\n",
    "        output = bertmodel(mask input)\n",
    "\n",
    "        # 4. training, loss \n",
    "        loss = cross entropy(output[mask position], labels[mask position])\n",
    "\n",
    "        # 5. backpropagation or update parameter\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero grad()\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "\n",
    "1. s1 = \"I love pussy\",\n",
    "\n",
    "2. s2 : \"langit berwarna biru hari ini\",\n",
    "\n",
    "3. Embedding :\n",
    "\n",
    "['[CLS]', 'i', 'love', 'pussy', '[SEP]', 'langit', 'berwarna', 'biru', 'hari', 'ini', '[SEP]']\n",
    "\n",
    "- [1, 4, 5, 6, 2, 7, 8, 9, 10, 11, 2] \n",
    "\n",
    "4. BERT Model :\n",
    "\n",
    "- MHA âž” AddNorm\n",
    "\n",
    "- FFN âž” AddNorm\n",
    "\n",
    "- logits + softmax -> cls\n",
    "- cls = [0.48, 0.45] \n",
    "- cls[0] = berhubungan\n",
    "- cls[1] = tidak berhubungan\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa36960",
   "metadata": {},
   "source": [
    "## PRETRAIN BERT MODEL NSP\n",
    "\n",
    "src = https://www.101ai.net/text/bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b261fc3",
   "metadata": {},
   "source": [
    "## Original From BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "68a95b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468405ae36ec42d481bdc2e7193f5af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770482d380a54db2bd985c5098cd16d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ce2dc838d4b8f97f7a77e6d3d0ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0d5eb6c0ca422392b955bd68d65274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ac6dde2de3445582e7a90fa638d986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5b26438e2c41d981a02f1777ba369a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcca96459c644f03958835637f519f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ac7bbe5d9f499e9befed1e1bdf4c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b550f0d890419eacd102f93c4e786b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749fc036166c465b96b094c6343814a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b72e91052a2490da2651790a5600aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 56.18%\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "s1 = \"aku suka makan nasi\"\n",
    "s2 = \"nasi itu enak\"\n",
    "\n",
    "emb1 = model.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "score = util.cos_sim(emb1, emb2).item()\n",
    "percentage = score * 100\n",
    "\n",
    "print(f\"Similarity: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e012469",
   "metadata": {},
   "source": [
    "## Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed39225-2666-465f-8b9a-798ee265751d",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aedfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a2648d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I love pussy', 'langit berwarna biru hari ini')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love pussy\",\n",
    "    \"langit berwarna biru hari ini\",\n",
    "    \"Langit berwarna biru hari ini\",\n",
    "    \"Dia sedang belajar matematika\"\n",
    "]\n",
    "\n",
    "sentences[0], sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcca6e6",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c15123ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'i': 4, 'love': 5, 'pussy': 6, 'langit': 12, 'berwarna': 13, 'biru': 14, 'hari': 15, 'ini': 16, 'dia': 17, 'sedang': 18, 'belajar': 19, 'matematika': 20}\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for sentence in sentences for word in sentence.lower().split()]\n",
    "\n",
    "vocab = {\n",
    "    \"[PAD]\": 0,\n",
    "    \"[CLS]\": 1,\n",
    "    \"[SEP]\": 2,\n",
    "    \"[MASK]\": 3,\n",
    "}\n",
    "\n",
    "for i, w in enumerate(tokens, start=len(vocab)):\n",
    "    vocab[w] = i\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76053bb0",
   "metadata": {},
   "source": [
    "### Tokens ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "23309196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'i': 4, 'love': 5, 'pussy': 6, 'langit': 7, 'berwarna': 8, 'biru': 9, 'hari': 10, 'ini': 11, 'dia': 12, 'sedang': 13, 'belajar': 14, 'matematika': 15}\n",
      "\n",
      "{0: '[PAD]', 1: '[CLS]', 2: '[SEP]', 3: '[MASK]', 4: 'i', 5: 'love', 6: 'pussy', 7: 'langit', 8: 'berwarna', 9: 'biru', 10: 'hari', 11: 'ini', 12: 'dia', 13: 'sedang', 14: 'belajar', 15: 'matematika'}\n",
      "\n",
      "[CLS]\n"
     ]
    }
   ],
   "source": [
    "t2i = {token : idx for idx, token in enumerate(vocab)}\n",
    "i2t = {idx : token for token, idx in t2i.items()}\n",
    "\n",
    "print(t2i)\n",
    "print()\n",
    "print(i2t)\n",
    "print()\n",
    "print(i2t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f787840",
   "metadata": {},
   "source": [
    "### Sentences + Its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "821caee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'pussy'] ['langit', 'berwarna', 'biru', 'hari', 'ini']\n",
      "\n",
      "['[CLS]', 'i', 'love', 'pussy', '[SEP]', 'langit', 'berwarna', 'biru', 'hari', 'ini', '[SEP]']\n",
      "\n",
      "[1, 4, 5, 6, 2, 7, 8, 9, 10, 11, 2] 11\n"
     ]
    }
   ],
   "source": [
    "s1 = sentences[0].lower().split()\n",
    "s2 = sentences[1].lower().split()\n",
    "print(s1, s2)\n",
    "print()\n",
    "\n",
    "tokens = [i2t[1]] + s1 + [i2t[2]] + s2 + [i2t[2]]\n",
    "ids = [t2i[tokens] for tokens in tokens]\n",
    "\n",
    "print(tokens)\n",
    "print()\n",
    "print(ids, len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dff562",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8cad6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.234 -0.234  1.579  0.767]\n",
      " [-1.013  0.314 -0.908 -1.412]\n",
      " [ 1.466 -0.226  0.068 -1.425]\n",
      " [-0.544  0.111 -1.151  0.376]\n",
      " [-0.469  0.543 -0.463 -0.466]\n",
      " [-0.601 -0.292 -0.602  1.852]\n",
      " [-0.013 -1.058  0.823 -1.221]\n",
      " [ 0.209 -1.96  -1.328  0.197]\n",
      " [ 0.738  0.171 -0.116 -0.301]\n",
      " [-1.479 -0.72  -0.461  1.057]\n",
      " [-0.469  0.543 -0.463 -0.466]] (11, 4)\n"
     ]
    }
   ],
   "source": [
    "dim = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "embedding_matrix = np.random.randn(len(t2i), dim)  # dummy emb\n",
    "input_emb = np.array([embedding_matrix[i] for i in ids])  # shape: (seq_len, dim)\n",
    "\n",
    "print(input_emb, input_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d61e69a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]      | ID: 1   | Embedding: [-0.234 -0.234  1.579  0.767]\n",
      "Token: i          | ID: 4   | Embedding: [-1.013  0.314 -0.908 -1.412]\n",
      "Token: love       | ID: 5   | Embedding: [ 1.466 -0.226  0.068 -1.425]\n",
      "Token: pussy      | ID: 6   | Embedding: [-0.544  0.111 -1.151  0.376]\n",
      "Token: [SEP]      | ID: 2   | Embedding: [-0.469  0.543 -0.463 -0.466]\n",
      "Token: langit     | ID: 7   | Embedding: [-0.601 -0.292 -0.602  1.852]\n",
      "Token: berwarna   | ID: 8   | Embedding: [-0.013 -1.058  0.823 -1.221]\n",
      "Token: biru       | ID: 9   | Embedding: [ 0.209 -1.96  -1.328  0.197]\n",
      "Token: hari       | ID: 10  | Embedding: [ 0.738  0.171 -0.116 -0.301]\n",
      "Token: ini        | ID: 11  | Embedding: [-1.479 -0.72  -0.461  1.057]\n",
      "Token: [SEP]      | ID: 2   | Embedding: [-0.469  0.543 -0.463 -0.466]\n"
     ]
    }
   ],
   "source": [
    "for token, idx in zip(tokens, ids):\n",
    "    emb = embedding_matrix[idx]\n",
    "    print(f\"Token: {token:<10} | ID: {idx:<3} | Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a750f",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a8334ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(t2i)\n",
    "\n",
    "pos = np.arange(max_len)[:, np.newaxis]\n",
    "i = np.arange(dim)[np.newaxis, :]\n",
    "\n",
    "angle_rates = 1 / np.power(10000, (2*(i//2))/dim)\n",
    "angle_rads = pos * angle_rates\n",
    "\n",
    "angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "positional_encoding = angle_rads\n",
    "\n",
    "# PE + EMBEDDING\n",
    "pe = input_emb + positional_encoding[:len(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "75401d71-61cd-4975-bd71-dde58d201483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]      | Pos: 0  | ID: 1   | Emb + PE: [-0.234  0.766  1.579  1.767]\n",
      "Token: i          | Pos: 1  | ID: 4   | Emb + PE: [-0.171  0.855 -0.898 -0.412]\n",
      "Token: love       | Pos: 2  | ID: 5   | Emb + PE: [ 2.375 -0.642  0.088 -0.425]\n",
      "Token: pussy      | Pos: 3  | ID: 6   | Emb + PE: [-0.403 -0.879 -1.121  1.375]\n",
      "Token: [SEP]      | Pos: 4  | ID: 2   | Emb + PE: [-1.226 -0.111 -0.423  0.533]\n",
      "Token: langit     | Pos: 5  | ID: 7   | Emb + PE: [-1.56  -0.008 -0.552  2.851]\n",
      "Token: berwarna   | Pos: 6  | ID: 8   | Emb + PE: [-0.293 -0.098  0.883 -0.223]\n",
      "Token: biru       | Pos: 7  | ID: 9   | Emb + PE: [ 0.866 -1.206 -1.258  1.194]\n",
      "Token: hari       | Pos: 8  | ID: 10  | Emb + PE: [ 1.728  0.026 -0.036  0.696]\n",
      "Token: ini        | Pos: 9  | ID: 11  | Emb + PE: [-1.066 -1.631 -0.371  2.053]\n",
      "Token: [SEP]      | Pos: 10 | ID: 2   | Emb + PE: [-1.013 -0.297 -0.364  0.529]\n"
     ]
    }
   ],
   "source": [
    "for pos, (token, idx) in enumerate(zip(tokens, ids)):\n",
    "    emb = embedding_matrix[idx]\n",
    "    pes = positional_encoding[pos]\n",
    "    result = emb + pes\n",
    "    print(f\"Token: {token:<10} | Pos: {pos:<2} | ID: {idx:<3} | Emb + PE: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b2861e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.234,  0.766,  1.579,  1.767],\n",
       "       [-0.171,  0.855, -0.898, -0.412],\n",
       "       [ 2.375, -0.642,  0.088, -0.425],\n",
       "       [-0.403, -0.879, -1.121,  1.375],\n",
       "       [-1.226, -0.111, -0.423,  0.533],\n",
       "       [-1.56 , -0.008, -0.552,  2.851],\n",
       "       [-0.293, -0.098,  0.883, -0.223],\n",
       "       [ 0.866, -1.206, -1.258,  1.194],\n",
       "       [ 1.728,  0.026, -0.036,  0.696],\n",
       "       [-1.066, -1.631, -0.371,  2.053],\n",
       "       [-1.013, -0.297, -0.364,  0.529]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e051a0",
   "metadata": {},
   "source": [
    "### Multihead Multiscale Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fd950412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "num_heads = 2\n",
    "seq_len = pe.shape[0]\n",
    "assert dim%num_heads == 0\n",
    "depth = dim//num_heads\n",
    "\n",
    "np.random.seed(0)\n",
    "wq = np.random.rand(dim, dim)\n",
    "wk = np.random.rand(dim, dim)\n",
    "wv = np.random.rand(dim, dim)\n",
    "wo = np.random.rand(dim, dim)\n",
    "\n",
    "q = pe @ wq\n",
    "k = pe @ wk\n",
    "v = pe @ wv\n",
    "\n",
    "q = q.reshape((seq_len, num_heads, depth)).transpose(1,0,2)\n",
    "k = k.reshape((seq_len, num_heads, depth)).transpose(1,0,2)\n",
    "v = v.reshape((seq_len, num_heads, depth)).transpose(1,0,2)\n",
    "\n",
    "dk = q.shape[-1]\n",
    "scores = np.matmul(q, k.transpose(0,2,1)) / np.sqrt(dk)\n",
    "\n",
    "if 'mask' in locals() and mask is not None:\n",
    "    scores += (mask * -1e9)\n",
    "\n",
    "attn_weights = softmax(scores, axis=-1)\n",
    "output = np.matmul(attn_weights, v)\n",
    "\n",
    "output = output.transpose(1,0,2).reshape((seq_len, dim))\n",
    "output = output @ wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "344a8148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== SHAPES ====\n",
      "q.shape           : (2, 11, 2)\n",
      "k.shape           : (2, 11, 2)\n",
      "v.shape           : (2, 11, 2)\n",
      "scores.shape      : (2, 11, 11)\n",
      "attn_weights.shape: (2, 11, 11)\n",
      "output.shape      : (11, 4)\n",
      "\n",
      "==== VALUES (truncated if too long) ====\n",
      "q.shape: (2, 11, 2)\n",
      "q:\n",
      "[[[ 2.722  2.569]\n",
      "  [-0.832 -0.297]\n",
      "  [ 0.874  0.924]\n",
      "  [-0.893 -0.013]\n",
      "  [-0.825 -0.617]\n",
      "  [ 0.229  1.307]\n",
      "  [ 0.522 -0.14 ]\n",
      "  [-0.57   0.464]\n",
      "  [ 1.32   1.883]\n",
      "  [-0.467 -0.058]\n",
      "  [-0.732 -0.566]]\n",
      "\n",
      " [[ 1.57   1.545]\n",
      "  [-0.47   0.158]\n",
      "  [ 1.19   0.731]\n",
      "  [-1.418 -1.477]\n",
      "  [-1.085 -0.945]\n",
      "  [-1.178 -0.9  ]\n",
      "  [ 0.464  0.201]\n",
      "  [-0.917 -1.165]\n",
      "  [ 1.074  1.006]\n",
      "  [-1.504 -2.053]\n",
      "  [-0.991 -0.963]]]\n",
      "\n",
      "k.shape: (2, 11, 2)\n",
      "k:\n",
      "[[[ 1.854  2.161]\n",
      "  [ 0.511 -0.205]\n",
      "  [-0.792  1.344]\n",
      "  [-0.283 -1.185]\n",
      "  [ 0.095 -1.16 ]\n",
      "  [ 1.383 -0.476]\n",
      "  [-0.113  0.151]\n",
      "  [-0.688 -0.553]\n",
      "  [ 0.419  1.725]\n",
      "  [-0.59  -1.577]\n",
      "  [-0.077 -1.094]]\n",
      "\n",
      " [[ 0.865  3.254]\n",
      "  [ 0.023 -0.65 ]\n",
      "  [ 1.452  1.319]\n",
      "  [-0.516 -1.031]\n",
      "  [-0.925 -1.141]\n",
      "  [-0.542  0.323]\n",
      "  [-0.205  0.33 ]\n",
      "  [ 0.253 -0.452]\n",
      "  [ 1.535  2.028]\n",
      "  [-1.092 -0.961]\n",
      "  [-0.838 -1.047]]]\n",
      "\n",
      "v.shape: (2, 11, 2)\n",
      "v:\n",
      "[[[ 2.108  2.215]\n",
      "  [-0.153 -0.239]\n",
      "  [ 0.439  0.707]\n",
      "  [-0.208 -0.339]\n",
      "  [-0.424 -0.593]\n",
      "  [ 0.986  0.779]\n",
      "  [-0.024  0.01 ]\n",
      "  [ 0.001 -0.001]\n",
      "  [ 1.255  1.449]\n",
      "  [-0.249 -0.398]\n",
      "  [-0.422 -0.563]]\n",
      "\n",
      " [[ 2.192  0.701]\n",
      "  [ 0.09   0.37 ]\n",
      "  [-0.59   0.98 ]\n",
      "  [-1.33  -0.739]\n",
      "  [-0.311 -0.79 ]\n",
      "  [ 0.178 -0.634]\n",
      "  [ 0.471 -0.223]\n",
      "  [-1.748 -0.209]\n",
      "  [ 0.178  1.172]\n",
      "  [-1.386 -1.528]\n",
      "  [-0.441 -0.782]]]\n",
      "\n",
      "scores.shape: (2, 11, 11)\n",
      "scores:\n",
      "[[[ 7.492  0.611  0.918 -2.698 -1.924  1.798  0.056 -2.328  3.94  -4.001 -2.136]\n",
      "  [-1.543 -0.258  0.184  0.415  0.187 -0.714  0.035  0.52  -0.608  0.678  0.275]\n",
      "  [ 2.558  0.182  0.389 -0.95  -0.699  0.544  0.028 -0.786  1.386 -1.396 -0.763]\n",
      "  [-1.19  -0.321  0.487  0.19  -0.049 -0.869  0.07   0.439 -0.281  0.387  0.059]\n",
      "  [-2.025 -0.209 -0.125  0.683  0.451 -0.599  0.     0.643 -0.997  1.033  0.523]\n",
      "  [ 2.296 -0.107  1.114 -1.141 -1.056 -0.216  0.121 -0.622  1.662 -1.553 -1.023]\n",
      "  [ 0.47   0.209 -0.425  0.013  0.15   0.558 -0.057 -0.199 -0.016 -0.061  0.08 ]\n",
      "  [-0.039 -0.273  0.759 -0.274 -0.418 -0.713  0.095  0.096  0.397 -0.279 -0.327]\n",
      "  [ 4.607  0.204  1.051 -1.842 -1.455  0.658  0.095 -1.378  2.687 -2.65  -1.529]\n",
      "  [-0.701 -0.161  0.206  0.142  0.016 -0.438  0.031  0.25  -0.209  0.26   0.07 ]\n",
      "  [-1.823 -0.182 -0.128  0.621  0.415 -0.525 -0.002  0.577 -0.907  0.936  0.478]]\n",
      "\n",
      " [[ 4.515 -0.684  3.052 -1.699 -2.273 -0.249  0.133 -0.213  3.92  -2.263 -2.073]\n",
      "  [ 0.076 -0.08  -0.335  0.056  0.18   0.216  0.105 -0.134 -0.284  0.256  0.161]\n",
      "  [ 2.41  -0.316  1.903 -0.967 -1.368 -0.289 -0.002 -0.021  2.34  -1.416 -1.246]\n",
      "  [-4.265  0.655 -2.833  1.594  2.118  0.206 -0.139  0.218 -3.657  2.099  1.933]\n",
      "  [-2.838  0.416 -1.995  1.085  1.472  0.2   -0.063  0.108 -2.533  1.481  1.342]\n",
      "  [-2.792  0.394 -2.049  1.087  1.497  0.246 -0.039  0.077 -2.57   1.522  1.364]\n",
      "  [ 0.746 -0.085  0.663 -0.316 -0.465 -0.132 -0.02   0.019  0.791 -0.495 -0.423]\n",
      "  [-3.242  0.52  -2.028  1.184  1.539  0.085 -0.139  0.208 -2.666  1.5    1.405]\n",
      "  [ 2.973 -0.445  2.041 -1.126 -1.514 -0.182  0.079 -0.129  2.609 -1.514 -1.381]\n",
      "  [-5.644  0.918 -3.459  2.046  2.639  0.108 -0.261  0.387 -4.577  2.558  2.41 ]\n",
      "  [-2.822  0.426 -1.915  1.064  1.425  0.16  -0.081  0.13  -2.457  1.42   1.3  ]]]\n",
      "\n",
      "attn_weights.shape: (2, 11, 11)\n",
      "attn_weights:\n",
      "[[[0.966 0.001 0.001 0.    0.    0.003 0.001 0.    0.028 0.    0.   ]\n",
      "  [0.018 0.065 0.101 0.127 0.101 0.041 0.087 0.141 0.046 0.165 0.11 ]\n",
      "  [0.529 0.049 0.06  0.016 0.02  0.071 0.042 0.019 0.164 0.01  0.019]\n",
      "  [0.027 0.065 0.146 0.108 0.085 0.038 0.096 0.139 0.068 0.132 0.095]\n",
      "  [0.01  0.059 0.064 0.145 0.115 0.04  0.073 0.139 0.027 0.205 0.123]\n",
      "  [0.435 0.039 0.133 0.014 0.015 0.035 0.049 0.023 0.23  0.009 0.016]\n",
      "  [0.131 0.101 0.054 0.083 0.095 0.143 0.078 0.067 0.081 0.077 0.089]\n",
      "  [0.088 0.07  0.195 0.07  0.06  0.045 0.101 0.101 0.136 0.069 0.066]\n",
      "  [0.815 0.01  0.023 0.001 0.002 0.016 0.009 0.002 0.12  0.001 0.002]\n",
      "  [0.046 0.078 0.113 0.106 0.093 0.059 0.095 0.118 0.075 0.119 0.099]\n",
      "  [0.012 0.063 0.067 0.141 0.115 0.045 0.076 0.135 0.031 0.193 0.122]]\n",
      "\n",
      " [[0.548 0.003 0.127 0.001 0.001 0.005 0.007 0.005 0.302 0.001 0.001]\n",
      "  [0.094 0.081 0.063 0.093 0.105 0.109 0.097 0.077 0.066 0.113 0.103]\n",
      "  [0.339 0.022 0.204 0.012 0.008 0.023 0.03  0.03  0.316 0.007 0.009]\n",
      "  [0.    0.057 0.002 0.146 0.247 0.036 0.026 0.037 0.001 0.242 0.205]\n",
      "  [0.003 0.074 0.007 0.144 0.211 0.059 0.046 0.054 0.004 0.213 0.186]\n",
      "  [0.003 0.071 0.006 0.141 0.213 0.061 0.046 0.051 0.004 0.218 0.186]\n",
      "  [0.166 0.073 0.153 0.058 0.05  0.069 0.077 0.08  0.174 0.048 0.052]\n",
      "  [0.002 0.078 0.006 0.151 0.216 0.05  0.04  0.057 0.003 0.208 0.189]\n",
      "  [0.432 0.014 0.17  0.007 0.005 0.018 0.024 0.019 0.3   0.005 0.006]\n",
      "  [0.    0.048 0.001 0.15  0.271 0.022 0.015 0.028 0.    0.25  0.215]\n",
      "  [0.003 0.077 0.007 0.145 0.209 0.059 0.046 0.057 0.004 0.208 0.184]]]\n",
      "\n",
      "output.shape: (11, 4)\n",
      "output:\n",
      "[[ 3.714  1.369  2.748  1.667]\n",
      " [-0.232 -0.09  -0.294 -0.097]\n",
      " [ 2.407  0.893  1.88   1.099]\n",
      " [-0.503 -0.236 -0.826 -0.24 ]\n",
      " [-0.735 -0.299 -0.892 -0.328]\n",
      " [ 1.198  0.345  0.188  0.507]\n",
      " [ 0.555  0.222  0.477  0.28 ]\n",
      " [-0.037 -0.077 -0.516 -0.038]\n",
      " [ 3.262  1.196  2.412  1.471]\n",
      " [-0.466 -0.222 -0.819 -0.221]\n",
      " [-0.704 -0.288 -0.867 -0.314]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tensor(name, tensor):\n",
    "    print(f\"{name}.shape: {tensor.shape}\")\n",
    "    print(f\"{name}:\\n{tensor}\\n\")\n",
    "\n",
    "print(\"==== SHAPES ====\")\n",
    "print(f\"q.shape           : {q.shape}\")\n",
    "print(f\"k.shape           : {k.shape}\")\n",
    "print(f\"v.shape           : {v.shape}\")\n",
    "print(f\"scores.shape      : {scores.shape}\")\n",
    "print(f\"attn_weights.shape: {attn_weights.shape}\")\n",
    "print(f\"output.shape      : {output.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"==== VALUES (truncated if too long) ====\")\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=120)\n",
    "\n",
    "print_tensor(\"q\", q)\n",
    "print_tensor(\"k\", k)\n",
    "print_tensor(\"v\", v)\n",
    "print_tensor(\"scores\", scores)\n",
    "print_tensor(\"attn_weights\", attn_weights)\n",
    "print_tensor(\"output\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "958e5fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token        | ID  | Before Attn                              | After Attn                              \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[CLS]        | 1   | [-0.23,  0.77,  1.58,  1.77]             | [3.71, 1.37, 2.75, 1.67]                \n",
      "i            | 4   | [-0.17,  0.85, -0.9 , -0.41]             | [-0.23, -0.09, -0.29, -0.1 ]            \n",
      "love         | 5   | [ 2.37, -0.64,  0.09, -0.42]             | [2.41, 0.89, 1.88, 1.1 ]                \n",
      "pussy        | 6   | [-0.4 , -0.88, -1.12,  1.38]             | [-0.5 , -0.24, -0.83, -0.24]            \n",
      "[SEP]        | 2   | [-1.23, -0.11, -0.42,  0.53]             | [-0.73, -0.3 , -0.89, -0.33]            \n",
      "langit       | 7   | [-1.56, -0.01, -0.55,  2.85]             | [1.2 , 0.35, 0.19, 0.51]                \n",
      "berwarna     | 8   | [-0.29, -0.1 ,  0.88, -0.22]             | [0.56, 0.22, 0.48, 0.28]                \n",
      "biru         | 9   | [ 0.87, -1.21, -1.26,  1.19]             | [-0.04, -0.08, -0.52, -0.04]            \n",
      "hari         | 10  | [ 1.73,  0.03, -0.04,  0.7 ]             | [3.26, 1.2 , 2.41, 1.47]                \n",
      "ini          | 11  | [-1.07, -1.63, -0.37,  2.05]             | [-0.47, -0.22, -0.82, -0.22]            \n",
      "[SEP]        | 2   | [-1.01, -0.3 , -0.36,  0.53]             | [-0.7 , -0.29, -0.87, -0.31]            \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Token':<12} | {'ID':<3} | {'Before Attn':<40} | {'After Attn':<40}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for i, (token, idx) in enumerate(zip(tokens, ids)):\n",
    "    bev = pe[i]\n",
    "    attn = output[i]\n",
    "    bev_str = np.array2string(bev, precision=2, separator=', ', suppress_small=True, max_line_width=40)\n",
    "    attn_str = np.array2string(attn, precision=2, separator=', ', suppress_small=True, max_line_width=40)\n",
    "    print(f\"{token:<12} | {idx:<3} | {bev_str:<40} | {attn_str:<40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc42a03",
   "metadata": {},
   "source": [
    "### Residual Connection + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6fd6e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, eps=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "residual_connection = norm(pe + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9ad4fbf1-44c9-4b0a-b7d8-5d65d593613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]      | ID: 1     | After Attn + Residual: [ 0.174 -1.544  1.255  0.115]\n",
      "Token: i          | ID: 4     | After Attn + Residual: [-0.097  1.563 -1.218 -0.248]\n",
      "Token: love       | ID: 5     | After Attn + Residual: [ 1.618 -0.942  0.027 -0.703]\n",
      "Token: pussy      | ID: 6     | After Attn + Residual: [-0.175 -0.359 -1.093  1.627]\n",
      "Token: [SEP]      | ID: 2     | After Attn + Residual: [-1.314  0.554 -0.536  1.296]\n",
      "Token: langit     | ID: 7     | After Attn + Residual: [-0.718 -0.264 -0.72   1.702]\n",
      "Token: berwarna   | ID: 8     | After Attn + Residual: [-0.356 -0.617  1.715 -0.742]\n",
      "Token: biru       | ID: 9     | After Attn + Residual: [ 0.858 -0.794 -1.179  1.115]\n",
      "Token: hari       | ID: 10    | After Attn + Residual: [ 1.646 -1.049 -0.224 -0.374]\n",
      "Token: ini        | ID: 11    | After Attn + Residual: [-0.575 -0.793 -0.342  1.71 ]\n",
      "Token: [SEP]      | ID: 2     | After Attn + Residual: [-1.226  0.338 -0.554  1.441]\n"
     ]
    }
   ],
   "source": [
    "for i, (token, idx) in enumerate(zip(tokens, ids)):\n",
    "    res = residual_connection[i]\n",
    "    print(f\"Token: {token:<10} | ID: {idx:<5} | After Attn + Residual: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1af2f1",
   "metadata": {},
   "source": [
    "### FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ede030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ukuran layer di BERT (base)\n",
    "ffn_hidden = 3072  # bisa ubah sesuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5a9b0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight FFN\n",
    "w1 = np.random.rand(dim, ffn_hidden)\n",
    "b1 = np.random.rand(ffn_hidden)\n",
    "w2 = np.random.rand(ffn_hidden, dim)\n",
    "b2 = np.random.rand(dim)\n",
    "\n",
    "# FFN\n",
    "ffn_intermediate = np.maximum(0, residual_connection @ w1 + b1)  # ReLU\n",
    "ffn_output = ffn_intermediate @ w2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fea0ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_connection2 = residual_connection + ffn_output\n",
    "norm2 = (residual_connection2 - residual_connection2.mean(axis=-1, keepdims=True)) / (residual_connection2.std(axis=-1, keepdims=True) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "19415879",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8873f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_vector = encoder_output[0]  # karena [CLS] di awal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "feba5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_head = np.random.rand(dim, 2)  # dua kelas: is_next / not_next\n",
    "logits = cls_vector @ nsp_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "19fcd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_probs = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97eb5e",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f62cac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS vector: [ 0.831 -0.099 -1.597  0.865]\n",
      "Logits: [ 0.846 -0.474]\n",
      "NSP Probabilities: [0.789 0.211]\n"
     ]
    }
   ],
   "source": [
    "print(\"CLS vector:\", cls_vector)\n",
    "print(\"Logits:\", logits)\n",
    "print(\"NSP Probabilities:\", nsp_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ea06a",
   "metadata": {},
   "source": [
    "### Probability of Next Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a7a3259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat lanjut: 78.9%\n",
      "Bukan lanjut: 21.1%\n"
     ]
    }
   ],
   "source": [
    "percentages = (nsp_probs * 100)\n",
    "print(f\"Kalimat lanjut: {percentages[0]:.1f}%\")\n",
    "print(f\"Bukan lanjut: {percentages[1]:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182f30a",
   "metadata": {},
   "source": [
    "## Full Program For Pretraining BERT Model NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dfe983b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def data_for_nsp(corpus_list):\n",
    "    \"\"\"\n",
    "    Process list of sentences for NSP task\n",
    "    Args:\n",
    "        corpus_list: List of sentences to process\n",
    "    Returns:\n",
    "        vocab: List of unique tokens in the corpus\n",
    "    \"\"\"\n",
    "    # Split each sentence into tokens and flatten\n",
    "    all_tokens = []\n",
    "    for sentence in corpus_list:\n",
    "        all_tokens.extend(sentence.split())\n",
    "    \n",
    "    # Create unique vocabulary\n",
    "    vocab = list(set(all_tokens))\n",
    "    # Add special tokens\n",
    "    vocab.extend(['[CLS]', '[SEP]', '[MASK]'])\n",
    "    return vocab\n",
    "\n",
    "def tokenizer(vocab):\n",
    "    \"\"\"Create mapping between tokens and indices\"\"\"\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    for idx, token in enumerate(vocab):\n",
    "        token2idx[token] = idx\n",
    "        idx2token[idx] = token\n",
    "    return token2idx, idx2token\n",
    "\n",
    "def create_nsp_example(corpus_list, is_next=True):\n",
    "    \"\"\"\n",
    "    Create an example for the NSP task\n",
    "    Args:\n",
    "        corpus_list: List of sentences to sample from\n",
    "        is_next: If True, sentences are consecutive; if False, random pair\n",
    "    Returns:\n",
    "        tokens: List of tokens for the NSP example\n",
    "        nsp_label: 1 if sentences are consecutive, 0 if not\n",
    "    \"\"\"\n",
    "    if len(corpus_list) < 2:\n",
    "        raise ValueError(\"Need at least 2 sentences for NSP task\")\n",
    "    \n",
    "    # Select random sentence as the first sentence\n",
    "    idx = random.randint(0, len(corpus_list) - 2)\n",
    "    sentence_a = corpus_list[idx].split()\n",
    "    \n",
    "    if is_next:\n",
    "        # If is_next is True, take the next sentence\n",
    "        sentence_b = corpus_list[idx + 1].split()\n",
    "        nsp_label = 1\n",
    "    else:\n",
    "        # If is_next is False, randomly select a sentence that's not the next one\n",
    "        random_idx = random.randint(0, len(corpus_list) - 1)\n",
    "        while random_idx == idx + 1 or random_idx == idx:\n",
    "            random_idx = random.randint(0, len(corpus_list) - 1)\n",
    "        sentence_b = corpus_list[random_idx].split()\n",
    "        nsp_label = 0\n",
    "    \n",
    "    # Construct the complete sequence with special tokens\n",
    "    tokens = ['[CLS]'] + sentence_a + ['[SEP]'] + sentence_b + ['[SEP]']\n",
    "    \n",
    "    # Create segment IDs (0 for first sentence, 1 for second sentence)\n",
    "    segment_ids = [0] * (len(sentence_a) + 2) + [1] * (len(sentence_b) + 1)\n",
    "    \n",
    "    return tokens, segment_ids, nsp_label\n",
    "\n",
    "def apply_mlm(tokens, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Apply masking for the MLM task\n",
    "    Args:\n",
    "        tokens: List of tokens\n",
    "        mask_prob: Probability of masking a token\n",
    "    Returns:\n",
    "        masked_tokens: Tokens with some replaced by [MASK]\n",
    "        mlm_labels: Original tokens for masked positions, -1 for unmasked\n",
    "    \"\"\"\n",
    "    masked_tokens = tokens.copy()\n",
    "    mlm_labels = [-1] * len(tokens)\n",
    "    \n",
    "    # Don't mask special tokens [CLS] and [SEP]\n",
    "    maskable_indices = [i for i, token in enumerate(tokens) \n",
    "                        if token not in ['[CLS]', '[SEP]']]\n",
    "    \n",
    "    # Number of tokens to mask\n",
    "    n_to_mask = max(1, int(mask_prob * len(maskable_indices)))\n",
    "    \n",
    "    # Randomly select indices to mask\n",
    "    mask_indices = random.sample(maskable_indices, n_to_mask)\n",
    "    \n",
    "    for idx in mask_indices:\n",
    "        # Store the original token as the label\n",
    "        mlm_labels[idx] = tokens[idx]\n",
    "        \n",
    "        rand = random.random()\n",
    "        if rand < 0.8:\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            masked_tokens[idx] = '[MASK]'\n",
    "        elif rand < 0.9:\n",
    "            # 10% of the time, replace with random word\n",
    "            masked_tokens[idx] = random.choice(tokens)\n",
    "        # 10% of the time, keep the original word\n",
    "    \n",
    "    return masked_tokens, mlm_labels\n",
    "\n",
    "def token_ids(tokens, token2idx):\n",
    "    \"\"\"Convert tokens to token IDs\"\"\"\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        token_ids.append(token2idx.get(token, token2idx.get('[UNK]', 0)))\n",
    "    return token_ids, len(token_ids)\n",
    "\n",
    "def embedding(scale, d_model, vocab_size):\n",
    "    \"\"\"Create token embedding matrix\"\"\"\n",
    "    np.random.seed(42)\n",
    "    embed_matrix = np.random.rand(vocab_size, d_model) * scale\n",
    "    return embed_matrix\n",
    "\n",
    "def segment_embedding(scale, d_model, n_segments=2):\n",
    "    \"\"\"Create segment embedding matrix\"\"\"\n",
    "    np.random.seed(43)\n",
    "    segment_embed_matrix = np.random.rand(n_segments, d_model) * scale\n",
    "    return segment_embed_matrix\n",
    "\n",
    "def position_embedding(max_seq_len, d_model):\n",
    "    \"\"\"Create position embedding matrix\"\"\"\n",
    "    pos = np.arange(max_seq_len).reshape(max_seq_len, 1)\n",
    "    i = np.arange(d_model)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros((max_seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return pe\n",
    "\n",
    "def embedding_output(token_embed, segment_embed, segment_ids, position_embed, seq_len):\n",
    "    \"\"\"Combine token, segment, and position embeddings\"\"\"\n",
    "    token_output = token_embed[:seq_len]\n",
    "    segment_output = np.array([segment_embed[segment_id] for segment_id in segment_ids[:seq_len]])\n",
    "    position_output = position_embed[:seq_len]\n",
    "    \n",
    "    return token_output + segment_output + position_output\n",
    "\n",
    "def attention_weights(d_model):\n",
    "    \"\"\"Initialize attention weights\"\"\"\n",
    "    Wq = np.random.randn(d_model, d_model) * 0.01\n",
    "    Wk = np.random.randn(d_model, d_model) * 0.01\n",
    "    Wv = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    return Wq, Wk, Wv\n",
    "\n",
    "def attention_output(x, Wq, Wk, Wv):\n",
    "    \"\"\"Compute self-attention\"\"\"\n",
    "    Q = x @ Wq\n",
    "    K = x @ Wk\n",
    "    V = x @ Wv\n",
    "    d_k = Q.shape[-1]\n",
    "    scaled = np.matmul(Q, K.transpose()) / np.sqrt(d_k)\n",
    "    softmax = np.exp(scaled) / np.sum(np.exp(scaled), axis=-1, keepdims=True)\n",
    "\n",
    "    return np.matmul(softmax, V)\n",
    "\n",
    "def add_and_norm(x, attention_output):\n",
    "    \"\"\"Add and normalize\"\"\"\n",
    "    eps = 1e-6\n",
    "    avg = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    norm = (x - avg) / (std + eps)\n",
    "    norm_output = attention_output + norm\n",
    "    return norm_output\n",
    "    \n",
    "def ffn_weights(d_model, d_ff):\n",
    "    \"\"\"Initialize feed-forward network weights\"\"\"\n",
    "    W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "    W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "    return W1, W2\n",
    "\n",
    "def ffn_output(norm_output, W1, W2):\n",
    "    \"\"\"Feed-forward network computation\"\"\"\n",
    "    ffn_output = np.matmul(norm_output, W1)\n",
    "    relu = np.maximum(0, ffn_output)\n",
    "    ffn_output = np.matmul(relu, W2)\n",
    "    return ffn_output\n",
    "\n",
    "def initialize_model(d_model, d_ff, vocab_size, n_segments=2):\n",
    "    \"\"\"Initialize all model parameters\"\"\"\n",
    "    Wq, Wk, Wv = attention_weights(d_model)\n",
    "    W1, W2 = ffn_weights(d_model, d_ff)\n",
    "    token_embed_matrix = embedding(0.01, d_model, vocab_size)\n",
    "    segment_embed_matrix = segment_embedding(0.01, d_model, n_segments)\n",
    "    \n",
    "    # MLM output layer\n",
    "    W_mlm_output = np.random.randn(d_model, vocab_size) * 0.01\n",
    "    b_mlm_output = np.zeros(vocab_size)\n",
    "    \n",
    "    # NSP output layer\n",
    "    W_nsp_output = np.random.randn(d_model, 2) * 0.01  # 2 classes for NSP\n",
    "    b_nsp_output = np.zeros(2)\n",
    "    \n",
    "    return {\n",
    "        'Wq': Wq, 'Wk': Wk, 'Wv': Wv,\n",
    "        'W1': W1, 'W2': W2,\n",
    "        'token_embed_matrix': token_embed_matrix,\n",
    "        'segment_embed_matrix': segment_embed_matrix,\n",
    "        'W_mlm_output': W_mlm_output,\n",
    "        'b_mlm_output': b_mlm_output,\n",
    "        'W_nsp_output': W_nsp_output,\n",
    "        'b_nsp_output': b_nsp_output\n",
    "    }\n",
    "\n",
    "def forward_pass(token_ids_input, segment_ids, seq_len, model_params, d_model, vocab_size):\n",
    "    \"\"\"\n",
    "    Forward pass through the BERT model\n",
    "    Returns MLM and NSP outputs\n",
    "    \"\"\"\n",
    "    # One-hot encode the tokens\n",
    "    input_matrix = np.zeros((seq_len, vocab_size))\n",
    "    for i, token_id in enumerate(token_ids_input[:seq_len]):\n",
    "        input_matrix[i, token_id] = 1\n",
    "    \n",
    "    # Get embeddings\n",
    "    token_embeds = input_matrix @ model_params['token_embed_matrix']\n",
    "    pos_embeds = position_embedding(seq_len, d_model)\n",
    "    embed_output = embedding_output(\n",
    "        token_embeds, \n",
    "        model_params['segment_embed_matrix'], \n",
    "        segment_ids[:seq_len], \n",
    "        pos_embeds, \n",
    "        seq_len\n",
    "    )\n",
    "\n",
    "    # Self-attention block\n",
    "    attn_output = attention_output(embed_output, model_params['Wq'], model_params['Wk'], model_params['Wv'])\n",
    "    norm_output = add_and_norm(embed_output, attn_output)\n",
    "    \n",
    "    # Feed-forward block\n",
    "    ffn_out = ffn_output(norm_output, model_params['W1'], model_params['W2'])\n",
    "    ffn_norm_output = add_and_norm(norm_output, ffn_out)\n",
    "\n",
    "    # MLM head: predict token for each position\n",
    "    mlm_logits = ffn_norm_output @ model_params['W_mlm_output'] + model_params['b_mlm_output']\n",
    "    mlm_probs = np.exp(mlm_logits) / np.sum(np.exp(mlm_logits), axis=-1, keepdims=True)\n",
    "    \n",
    "    # NSP head: use [CLS] token representation for next sentence prediction\n",
    "    cls_output = ffn_norm_output[0]  # [CLS] token is at the beginning\n",
    "    nsp_logits = cls_output @ model_params['W_nsp_output'] + model_params['b_nsp_output']\n",
    "    nsp_probs = np.exp(nsp_logits) / np.sum(np.exp(nsp_logits))\n",
    "\n",
    "    return mlm_probs, nsp_probs, ffn_norm_output\n",
    "\n",
    "def compute_loss(mlm_probs, mlm_labels, nsp_probs, nsp_label, token2idx):\n",
    "    \"\"\"\n",
    "    Compute combined loss for MLM and NSP tasks\n",
    "    Args:\n",
    "        mlm_probs: Predicted probabilities for masked tokens\n",
    "        mlm_labels: Original tokens for masked positions, -1 for unmasked\n",
    "        nsp_probs: Predicted probabilities for NSP\n",
    "        nsp_label: True label for NSP (0 or 1)\n",
    "    \"\"\"\n",
    "    # MLM loss: only for masked positions\n",
    "    mlm_loss = 0\n",
    "    mask_count = 0\n",
    "    \n",
    "    for i, label in enumerate(mlm_labels):\n",
    "        if label != -1:  # If this position was masked\n",
    "            label_id = token2idx.get(label, 0)\n",
    "            mlm_loss -= np.log(mlm_probs[i, label_id] + 1e-10)\n",
    "            mask_count += 1\n",
    "    \n",
    "    if mask_count > 0:\n",
    "        mlm_loss /= mask_count\n",
    "    \n",
    "    # NSP loss\n",
    "    nsp_loss = -np.log(nsp_probs[nsp_label] + 1e-10)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = mlm_loss + nsp_loss\n",
    "    \n",
    "    return total_loss, mlm_loss, nsp_loss\n",
    "\n",
    "def backward_propagation(loss, mlm_probs, mlm_labels, nsp_probs, nsp_label, ffn_norm_output, \n",
    "                         model_params, token_ids_input, segment_ids, token2idx, lr=0.01):\n",
    "    \"\"\"Basic backward propagation to update model parameters\"\"\"\n",
    "    # Gradients for MLM output layer\n",
    "    d_W_mlm_output = np.zeros_like(model_params['W_mlm_output'])\n",
    "    d_b_mlm_output = np.zeros_like(model_params['b_mlm_output'])\n",
    "    \n",
    "    # Compute gradients for masked positions\n",
    "    for i, label in enumerate(mlm_labels):\n",
    "        if label != -1:  # If this position was masked\n",
    "            label_id = token2idx.get(label, 0)\n",
    "            d_probs = np.zeros_like(mlm_probs[i])\n",
    "            d_probs[label_id] = -1.0 / (mlm_probs[i, label_id] + 1e-10)\n",
    "            d_logits = mlm_probs[i] * d_probs\n",
    "            \n",
    "            d_W_mlm_output += np.outer(ffn_norm_output[i], d_logits)\n",
    "            d_b_mlm_output += d_logits\n",
    "    \n",
    "    # Gradients for NSP output layer\n",
    "    d_W_nsp_output = np.zeros_like(model_params['W_nsp_output'])\n",
    "    d_b_nsp_output = np.zeros_like(model_params['b_nsp_output'])\n",
    "    \n",
    "    d_nsp_probs = np.zeros_like(nsp_probs)\n",
    "    d_nsp_probs[nsp_label] = -1.0 / (nsp_probs[nsp_label] + 1e-10)\n",
    "    d_nsp_logits = nsp_probs * d_nsp_probs\n",
    "    \n",
    "    d_W_nsp_output += np.outer(ffn_norm_output[0], d_nsp_logits)  # [CLS] token\n",
    "    d_b_nsp_output += d_nsp_logits\n",
    "    \n",
    "    # Update parameters\n",
    "    model_params['W_mlm_output'] -= lr * d_W_mlm_output\n",
    "    model_params['b_mlm_output'] -= lr * d_b_mlm_output\n",
    "    model_params['W_nsp_output'] -= lr * d_W_nsp_output\n",
    "    model_params['b_nsp_output'] -= lr * d_b_nsp_output\n",
    "\n",
    "    return model_params\n",
    "\n",
    "def train_bert(corpus_list, n_epoch=1000):\n",
    "    \"\"\"\n",
    "    Train BERT with both MLM and NSP objectives\n",
    "    Args:\n",
    "        corpus_list: List of sentences\n",
    "        n_epoch: Number of training epochs\n",
    "    \"\"\"\n",
    "    # Model dimensions\n",
    "    d_model = 16\n",
    "    d_ff = 64\n",
    "    \n",
    "    # Process corpus\n",
    "    vocab = data_for_nsp(corpus_list)\n",
    "    vocab_size = len(vocab)\n",
    "    token2idx, idx2token = tokenizer(vocab)\n",
    "    \n",
    "    # Initialize model\n",
    "    model_params = initialize_model(d_model, d_ff, vocab_size)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = {\"total\": [], \"mlm\": [], \"nsp\": []}\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        # Create training example\n",
    "        is_next = random.choice([True, False])\n",
    "        tokens, segment_ids, nsp_label = create_nsp_example(corpus_list, is_next)\n",
    "        \n",
    "        # Apply masking for MLM\n",
    "        masked_tokens, mlm_labels = apply_mlm(tokens)\n",
    "        \n",
    "        # Convert to token IDs\n",
    "        token_ids_input, seq_len = token_ids(masked_tokens, token2idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        mlm_probs, nsp_probs, ffn_norm_output = forward_pass(\n",
    "            token_ids_input, segment_ids, seq_len, model_params, d_model, vocab_size\n",
    "        )\n",
    "        \n",
    "        # Calculate loss\n",
    "        total_loss, mlm_loss, nsp_loss = compute_loss(\n",
    "            mlm_probs, mlm_labels, nsp_probs, nsp_label, token2idx\n",
    "        )\n",
    "        \n",
    "        losses[\"total\"].append(total_loss)\n",
    "        losses[\"mlm\"].append(mlm_loss)\n",
    "        losses[\"nsp\"].append(nsp_loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        model_params = backward_propagation(\n",
    "            total_loss, mlm_probs, mlm_labels, nsp_probs, nsp_label, ffn_norm_output,\n",
    "            model_params, token_ids_input, segment_ids, token2idx, lr=0.01\n",
    "        )\n",
    "        \n",
    "        # Logging\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epoch}\")\n",
    "            print(f\"Total Loss: {total_loss:.4f}, MLM Loss: {mlm_loss:.4f}, NSP Loss: {nsp_loss:.4f}\")\n",
    "            \n",
    "            # MLM accuracy check: predict random masked token\n",
    "            mask_indices = [i for i, label in enumerate(mlm_labels) if label != -1]\n",
    "            if mask_indices:\n",
    "                random_mask_idx = random.choice(mask_indices)\n",
    "                predicted_token_id = np.argmax(mlm_probs[random_mask_idx])\n",
    "                print(f\"MLM sample - Predicted: {idx2token[predicted_token_id]}, Target: {mlm_labels[random_mask_idx]}\")\n",
    "            \n",
    "            # NSP accuracy check\n",
    "            predicted_nsp = np.argmax(nsp_probs)\n",
    "            print(f\"NSP - Predicted: {predicted_nsp}, Target: {nsp_label}\")\n",
    "            print()\n",
    "        \n",
    "    # Final evaluation\n",
    "    correct_nsp = 0\n",
    "    total_tests = 100\n",
    "    \n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    for _ in range(total_tests):\n",
    "        is_next = random.choice([True, False])\n",
    "        tokens, segment_ids, nsp_label = create_nsp_example(corpus_list, is_next)\n",
    "        masked_tokens, mlm_labels = apply_mlm(tokens)\n",
    "        token_ids_input, seq_len = token_ids(masked_tokens, token2idx)\n",
    "        \n",
    "        mlm_probs, nsp_probs, _ = forward_pass(\n",
    "            token_ids_input, segment_ids, seq_len, model_params, d_model, vocab_size\n",
    "        )\n",
    "        \n",
    "        predicted_nsp = np.argmax(nsp_probs)\n",
    "        if predicted_nsp == nsp_label:\n",
    "            correct_nsp += 1\n",
    "    \n",
    "    print(f\"NSP Accuracy: {correct_nsp/total_tests:.2%}\")\n",
    "    \n",
    "    return model_params, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "585bc29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000\n",
      "Total Loss: 7.9209, MLM Loss: 3.9613, NSP Loss: 3.9596\n",
      "MLM sample - Predicted: the, Target: i\n",
      "NSP - Predicted: 1, Target: 0\n",
      "\n",
      "Epoch 200/1000\n",
      "Total Loss: 4.3164, MLM Loss: 4.2971, NSP Loss: 0.0193\n",
      "MLM sample - Predicted: is, Target: networks\n",
      "NSP - Predicted: 1, Target: 1\n",
      "\n",
      "Epoch 300/1000\n",
      "Total Loss: 9.2947, MLM Loss: 4.3278, NSP Loss: 4.9669\n",
      "MLM sample - Predicted: is, Target: many\n",
      "NSP - Predicted: 1, Target: 0\n",
      "\n",
      "Epoch 400/1000\n",
      "Total Loss: 9.8398, MLM Loss: 3.5188, NSP Loss: 6.3210\n",
      "MLM sample - Predicted: is, Target: to\n",
      "NSP - Predicted: 1, Target: 0\n",
      "\n",
      "Epoch 500/1000\n",
      "Total Loss: 5.2079, MLM Loss: 5.2009, NSP Loss: 0.0070\n",
      "MLM sample - Predicted: is, Target: fun\n",
      "NSP - Predicted: 1, Target: 1\n",
      "\n",
      "Epoch 600/1000\n",
      "Total Loss: 5.1462, MLM Loss: 5.1460, NSP Loss: 0.0002\n",
      "MLM sample - Predicted: is, Target: around\n",
      "NSP - Predicted: 1, Target: 1\n",
      "\n",
      "Epoch 700/1000\n",
      "Total Loss: 7.7294, MLM Loss: 0.0504, NSP Loss: 7.6790\n",
      "MLM sample - Predicted: is, Target: is\n",
      "NSP - Predicted: 1, Target: 0\n",
      "\n",
      "Epoch 800/1000\n",
      "Total Loss: 13.8029, MLM Loss: 6.4636, NSP Loss: 7.3394\n",
      "MLM sample - Predicted: is, Target: happy\n",
      "NSP - Predicted: 1, Target: 0\n",
      "\n",
      "Epoch 900/1000\n",
      "Total Loss: 6.7540, MLM Loss: 6.7522, NSP Loss: 0.0018\n",
      "MLM sample - Predicted: is, Target: happy\n",
      "NSP - Predicted: 1, Target: 1\n",
      "\n",
      "Epoch 1000/1000\n",
      "Total Loss: 5.3105, MLM Loss: 5.3103, NSP Loss: 0.0002\n",
      "MLM sample - Predicted: is, Target: to\n",
      "NSP - Predicted: 1, Target: 1\n",
      "\n",
      "\n",
      "Final Evaluation:\n",
      "NSP Accuracy: 51.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corpus_list = [\n",
    "        \"the cat sat on the mat\",\n",
    "        \"it was happy there\",\n",
    "        \"dogs chase cats around\",\n",
    "        \"the weather is nice today\",\n",
    "        \"i like to read books\",\n",
    "        \"programming is fun to learn\",\n",
    "        \"artificial intelligence is advancing quickly\",\n",
    "        \"neural networks can solve many problems\"\n",
    "    ]\n",
    "    \n",
    "    model_params, losses = train_bert(corpus_list, n_epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f3476",
   "metadata": {},
   "source": [
    "| Step | Penjelasan                           | Status |\n",
    "| :--: | :----------------------------------- | :----: |\n",
    "|   1  | Bangun Mini-BERT Stack               |    âœ…   |\n",
    "|   2  | Pretraining (Masked LM + NSP)        |   âœ…    |\n",
    "|   3  | Fine-tuning ke task spesifik         |   ðŸ”œ   |\n",
    "|   4  | Buat dataset dummy buat latihan      |   ðŸ”œ   |\n",
    "|   5  | Build mindset & intuition level dewa |   ðŸ”œ   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739f297",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
