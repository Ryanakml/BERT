{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# =================== Hyperparams ====================\n",
    "np.random.seed(42)\n",
    "vocab = {}\n",
    "reverse_vocab = []\n",
    "vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "max_len = 16\n",
    "\n",
    "\n",
    "# =================== Tokenizer ====================\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    global vocab, reverse_vocab\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        words.update(simple_tokenize(s))\n",
    "    vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "    for i, w in enumerate(sorted(words), start=4):\n",
    "        vocab[w] = i\n",
    "    reverse_vocab = [k for k, v in sorted(vocab.items(), key=lambda x: x[1])]\n",
    "\n",
    "def encode(s1, s2):\n",
    "    ids = [vocab[\"[CLS]\"]] + [vocab[w] for w in simple_tokenize(s1)] + [vocab[\"[SEP]\"]]\n",
    "    ids += [vocab[w] for w in simple_tokenize(s2)] + [vocab[\"[SEP]\"]]\n",
    "    ids += [vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "    return np.array(ids[:max_len])\n",
    "\n",
    "\n",
    "# =================== Positional Encoding ====================\n",
    "def positional_encoding(max_len, dim):\n",
    "    pos = np.arange(max_len)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return angle_rads\n",
    "\n",
    "\n",
    "# =================== Layer Norm ====================\n",
    "def norm(x, eps=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "\n",
    "# =================== Attention ====================\n",
    "def attention(q, k, v):\n",
    "    dk = q.shape[-1]\n",
    "    scores = q @ k.T / np.sqrt(dk)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return weights @ v\n",
    "\n",
    "\n",
    "# =================== Multi-Head Attention ====================\n",
    "class MiniMHA:\n",
    "    def __init__(self, dim):\n",
    "        self.Wq = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wk = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wv = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wo = np.random.randn(dim, dim) * 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = x @ self.Wq\n",
    "        k = x @ self.Wk\n",
    "        v = x @ self.Wv\n",
    "        attn = attention(q, k, v)\n",
    "        out = attn @ self.Wo\n",
    "        return out\n",
    "\n",
    "\n",
    "# =================== Feed Forward ====================\n",
    "class MiniFFN:\n",
    "    def __init__(self, dim):\n",
    "        self.W1 = np.random.randn(dim, dim * 2) * 0.01\n",
    "        self.b1 = np.zeros(dim * 2)\n",
    "        self.W2 = np.random.randn(dim * 2, dim) * 0.01\n",
    "        self.b2 = np.zeros(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        return h @ self.W2 + self.b2\n",
    "\n",
    "\n",
    "# =================== Mini BERT Encoder ====================\n",
    "class MiniBERTEncoder:\n",
    "    def __init__(self, vocab_size, dim, max_len):\n",
    "        self.embedding = np.random.randn(vocab_size, dim) * 0.01\n",
    "        self.pos_encoding = positional_encoding(max_len, dim)\n",
    "        self.mha = MiniMHA(dim)\n",
    "        self.ffn = MiniFFN(dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding[input_ids] + self.pos_encoding[:len(input_ids)]\n",
    "        attn_out = self.mha.forward(x)\n",
    "        x = norm(x + attn_out)\n",
    "        ffn_out = self.ffn.forward(x)\n",
    "        x = norm(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =================== NSP Head ====================\n",
    "class NSPHead:\n",
    "    def __init__(self, dim):\n",
    "        self.W = np.random.randn(dim, 2) * 0.01\n",
    "        self.b = np.zeros(2)\n",
    "\n",
    "    def forward(self, cls_vec):\n",
    "        logits = cls_vec @ self.W + self.b\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probs\n",
    "\n",
    "\n",
    "# =================== Full BERT NSP Model ====================\n",
    "class MiniBERTForNSP:\n",
    "    def __init__(self, vocab_size, dim, max_len):\n",
    "        self.encoder = MiniBERTEncoder(vocab_size, dim, max_len)\n",
    "        self.nsp = NSPHead(dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.encoder.forward(input_ids)\n",
    "        cls_vec = x[0]\n",
    "        return self.nsp.forward(cls_vec)\n",
    "\n",
    "\n",
    "# =================== Sample Sentences & Testing ====================\n",
    "sentences = [\n",
    "    \"Kucing tidur di atas sofa\",\n",
    "    \"Anjing bermain di taman\",\n",
    "    \"Langit berwarna biru hari ini\",\n",
    "    \"Dia sedang belajar matematika\"\n",
    "]\n",
    "\n",
    "build_vocab(sentences)\n",
    "\n",
    "# Pasangan yang benar (IsNext)\n",
    "s1 = \"Kucing tidur di atas sofa\"\n",
    "s2 = \"Langit berwarna biru hari ini\"\n",
    "\n",
    "# Pasangan acak (NotNext)\n",
    "s3 = \"Kucing tidur di atas sofa\"\n",
    "s4 = \"Dia sedang belajar matematika\"\n",
    "\n",
    "input_ids_true = encode(s1, s2)\n",
    "input_ids_false = encode(s3, s4)\n",
    "\n",
    "model = MiniBERTForNSP(vocab_size=len(vocab), dim=embedding_dim, max_len=max_len)\n",
    "\n",
    "print(\"Input TRUE:\", s1, \" + \", s2)\n",
    "print(\"NSP Output Prob (IsNext vs NotNext):\", model.forward(input_ids_true))\n",
    "\n",
    "print(\"\\nInput FALSE:\", s3, \" + \", s4)\n",
    "print(\"NSP Output Prob (IsNext vs NotNext):\", model.forward(input_ids_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for s1, s2, _ in dataset:\n",
    "    tokens.update(s1.lower().split())\n",
    "    tokens.update(s2.lower().split())\n",
    "\n",
    "tokens = sorted(list(tokens))\n",
    "vocab = {w: i+2 for i, w in enumerate(tokens)}  # 0 = PAD, 1 = CLS\n",
    "vocab[\"[PAD]\"] = 0\n",
    "vocab[\"[CLS]\"] = 1\n",
    "vocab[\"[SEP]\"] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "s1 = \"Aku pergi ke toko\".lower().split()\n",
    "s2 = \"Lalu aku membeli roti\".lower().split()\n",
    "\n",
    "input_ids = [vocab[\"[CLS]\"]] + [vocab[w] for w in s1] + [vocab[\"[SEP]\"]] + [vocab[w] for w in s2] + [vocab[\"[SEP]\"]]\n",
    "max_len = 16\n",
    "\n",
    "if len(input_ids) < max_len:\n",
    "    input_ids += [vocab[\"[PAD]\"]] * (max_len - len(input_ids))\n",
    "else:\n",
    "    input_ids = input_ids[:max_len]\n",
    "\n",
    "input_ids = np.array(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Kita pakai Mini-BERT yang sebelumnya sudah kita bangun\n",
    "hidden_states = mini_bert_forward(input_ids)\n",
    "\n",
    "# Ambil output vector dari token [CLS] (posisi pertama)\n",
    "cls_vector = hidden_states[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = cls_vector.shape[0]  # misal 32 atau 64, tergantung BERT\n",
    "\n",
    "W = np.random.randn(hidden_dim, 2) * 0.01\n",
    "b = np.zeros(2)\n",
    "\n",
    "logits = cls_vector @ W + b\n",
    "\n",
    "# Softmax\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "pred = np.argmax(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(d_model):\n",
    "    token_result = []\n",
    "    print()\n",
    "    print(f\"Token {i}: {x[i]}\")\n",
    "    for j in range(0, d_model, 2):\n",
    "        xx = x[i][j]\n",
    "        yy = x[i][j + 1]\n",
    "        print(f\"Before Pos : (x,y) ({j}, {j+1}): ({xx}, {yy})\")\n",
    "\n",
    "        m = j\n",
    "        wi = 1/10000**(2*i/d_model)\n",
    "        thetha = m*wi\n",
    "\n",
    "        xx_r = xx*np.cos(thetha) - yy*np.sin(thetha)\n",
    "        yy_r = yy*np.sin(thetha) + yy*np.cos(thetha)\n",
    "\n",
    "        print(f\"After Pos : (x,y) ({j}, {j+1}): ({xx_r}, {yy_r})\")\n",
    "\n",
    "        # Store the results\n",
    "        result[i][j] = xx_r\n",
    "        if j + 1 < d_model: \n",
    "            result[i][j + 1] = yy_r\n",
    "                \n",
    "        token_result.extend([xx_r, yy_r])\n",
    "        \n",
    "    print(f\"Token {i} with positional encoding: {result[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Format: (kalimat, label) — label 1 = positif, 0 = negatif\n",
    "data = [\n",
    "    (\"aku suka banget sama film ini\", 1),\n",
    "    (\"filmnya bener-bener membosankan\", 0),\n",
    "    (\"aktingnya luar biasa\", 1),\n",
    "    (\"ngantuk banget pas nonton\", 0),\n",
    "    (\"ceritanya bikin terharu\", 1),\n",
    "    (\"gak masuk akal dan jelek\", 0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f\"\\n[Batch {b}]\")\n",
    "    sentence_ids = input[b]\n",
    "    sentence_words = [id2w.get(i, \"[UNK]\") for i in sentence_ids]\n",
    "    print(\"Kalimat:\", \" \".join(sentence_words))\n",
    "    print(\"Token IDs:\", sentence_ids)\n",
    "    \n",
    "    for h in range(num_heads):\n",
    "        print(f\"\\n  Head {h}:\")\n",
    "        for q in range(seq_len):\n",
    "            q_word = id2w.get(sentence_ids[q], \"[UNK]\")\n",
    "            print(f\"    Query Token {q:2d} [{q_word:<10}]\")\n",
    "\n",
    "            for k in range(seq_len):\n",
    "                k_word = id2w.get(sentence_ids[k], \"[UNK]\")\n",
    "                score = scores[b, h, q, k]\n",
    "                weight = att_weights[b, h, q, k]\n",
    "                print(f\"      ↳ Key Token {k:2d} [{k_word:<10}] | Score: {score:>7.4f} | Softmax: {weight:>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2 contoh kalimat + label (1 = positif, 0 = negatif)\n",
    "inputs = np.array([\n",
    "    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],  # \"aku suka banget film ini\"\n",
    "    [1, 6, 7, 8, 9, 0, 0, 0, 0, 0]   # \"aku benci banget endingnya\"\n",
    "])\n",
    "labels = np.array([1, 0])\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "vocab_size = 20\n",
    "embedding_dim = 8\n",
    "head_dim = 4\n",
    "num_heads = 2\n",
    "ff_hidden = 32\n",
    "max_len = 10\n",
    "\n",
    "# Word Embedding\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim)\n",
    "\n",
    "# Positional Encoding\n",
    "pos_embed = np.random.randn(max_len, embedding_dim)\n",
    "\n",
    "# MHA Proj\n",
    "W_q = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_k = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_v = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_o = np.random.randn(num_heads * head_dim, embedding_dim)\n",
    "\n",
    "# FFN\n",
    "W1 = np.random.randn(embedding_dim, ff_hidden)\n",
    "b1 = np.zeros(ff_hidden)\n",
    "W2 = np.random.randn(ff_hidden, embedding_dim)\n",
    "b2 = np.zeros(embedding_dim)\n",
    "\n",
    "# Classifier\n",
    "W_cls = np.random.randn(embedding_dim, 1)\n",
    "b_cls = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Sentiment Analysis\n",
    "\n",
    "1. data -> sentence and label\n",
    "2. vocab : add cls and pad in index 1 and 0\n",
    "3. tokenize : tokens the sentence and add cls and pad\n",
    "4. input numeric and labels : sentence, token numeric, dim token and labels\n",
    "5. embedding : seq_len, dim_token\n",
    "6. positional encoding : seq_len, dim_token\n",
    "7. pe = positional encoding + embedding\n",
    "8. multihead attention : batch_size, seq_len, dim_token, num_heads, head_dim\n",
    "9. feed forward : batch_size, seq_len, dim_token, dim_ffn\n",
    "10. logits : batch_size, seq_len, num_classes\n",
    "11. loss : cross entropy\n",
    "12. update parameter : gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "for i in range(2):  # dua data\n",
    "    x = inputs[i]        # input token\n",
    "    y = labels[i]        # label asli\n",
    "\n",
    "    # ----- EMBEDDING -----\n",
    "    embed = W_embed[x]                       # (10, 8)\n",
    "    x_embed = embed + pos_embed              # (10, 8)\n",
    "\n",
    "    # ----- MHA -----\n",
    "    Q = np.einsum('ij,hjk->hik', x_embed, W_q)\n",
    "    K = np.einsum('ij,hjk->hik', x_embed, W_k)\n",
    "    V = np.einsum('ij,hjk->hik', x_embed, W_v)\n",
    "    att_scores = np.einsum('hij,hkj->hik', Q, K) / np.sqrt(head_dim)\n",
    "    att_weights = np.exp(att_scores - np.max(att_scores, axis=-1, keepdims=True))\n",
    "    att_weights /= np.sum(att_weights, axis=-1, keepdims=True)\n",
    "    att_out = np.einsum('hij,hjk->hik', att_weights, V)\n",
    "    att_concat = np.concatenate([att_out[0], att_out[1]], axis=-1)\n",
    "    att_projected = att_concat @ W_o\n",
    "\n",
    "    # Add & Norm\n",
    "    def norm(x):\n",
    "        eps = 1e-6\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + eps)\n",
    "\n",
    "    res1 = x_embed + att_projected\n",
    "    norm1 = norm(res1)\n",
    "\n",
    "    # ----- FFN -----\n",
    "    ff = np.maximum(0, norm1 @ W1 + b1) @ W2 + b2\n",
    "\n",
    "    # Add & Norm\n",
    "    res2 = norm1 + ff\n",
    "    norm2 = norm(res2)\n",
    "\n",
    "    # ----- CLS Token -----\n",
    "    cls_token = norm2[0]\n",
    "    logit = cls_token @ W_cls + b_cls\n",
    "    prob = 1 / (1 + np.exp(-logit))  # sigmoid\n",
    "    loss = -(y * np.log(prob + 1e-6) + (1 - y) * np.log(1 - prob + 1e-6))\n",
    "\n",
    "    print(f\"[{i}] Prob: {prob}, Label: {y}, Loss: {loss}\")\n",
    "\n",
    "    # ----- BACKPROP (manual grad descent, simple) -----\n",
    "    dlogit = prob - y  # deriv dari sigmoid + BCE\n",
    "    dW_cls = cls_token[:, None] * dlogit  # outer product\n",
    "    db_cls = dlogit\n",
    "\n",
    "    # Update weights\n",
    "    W_cls -= lr * dW_cls\n",
    "    b_cls -= lr * db_cls\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
