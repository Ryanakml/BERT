{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input TRUE: Kucing tidur di atas sofa  +  Langit berwarna biru hari ini\n",
      "NSP Output Prob (IsNext vs NotNext): [0.48686526 0.51313474]\n",
      "\n",
      "Input FALSE: Kucing tidur di atas sofa  +  Dia sedang belajar matematika\n",
      "NSP Output Prob (IsNext vs NotNext): [0.48686528 0.51313472]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# =================== Hyperparams ====================\n",
    "np.random.seed(42)\n",
    "vocab = {}\n",
    "reverse_vocab = []\n",
    "vocab_size = 1000\n",
    "embedding_dim = 32\n",
    "max_len = 16\n",
    "\n",
    "\n",
    "# =================== Tokenizer ====================\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().replace(\".\", \"\").replace(\",\", \"\").split()\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    global vocab, reverse_vocab\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        words.update(simple_tokenize(s))\n",
    "    vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[MASK]\": 3}\n",
    "    for i, w in enumerate(sorted(words), start=4):\n",
    "        vocab[w] = i\n",
    "    reverse_vocab = [k for k, v in sorted(vocab.items(), key=lambda x: x[1])]\n",
    "\n",
    "def encode(s1, s2):\n",
    "    ids = [vocab[\"[CLS]\"]] + [vocab[w] for w in simple_tokenize(s1)] + [vocab[\"[SEP]\"]]\n",
    "    ids += [vocab[w] for w in simple_tokenize(s2)] + [vocab[\"[SEP]\"]]\n",
    "    ids += [vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "    return np.array(ids[:max_len])\n",
    "\n",
    "\n",
    "# =================== Positional Encoding ====================\n",
    "def positional_encoding(max_len, dim):\n",
    "    pos = np.arange(max_len)[:, None]\n",
    "    i = np.arange(dim)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return angle_rads\n",
    "\n",
    "\n",
    "# =================== Layer Norm ====================\n",
    "def norm(x, eps=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "\n",
    "# =================== Attention ====================\n",
    "def attention(q, k, v):\n",
    "    dk = q.shape[-1]\n",
    "    scores = q @ k.T / np.sqrt(dk)\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "    return weights @ v\n",
    "\n",
    "\n",
    "# =================== Multi-Head Attention ====================\n",
    "class MiniMHA:\n",
    "    def __init__(self, dim):\n",
    "        self.Wq = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wk = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wv = np.random.randn(dim, dim) * 0.01\n",
    "        self.Wo = np.random.randn(dim, dim) * 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = x @ self.Wq\n",
    "        k = x @ self.Wk\n",
    "        v = x @ self.Wv\n",
    "        attn = attention(q, k, v)\n",
    "        out = attn @ self.Wo\n",
    "        return out\n",
    "\n",
    "\n",
    "# =================== Feed Forward ====================\n",
    "class MiniFFN:\n",
    "    def __init__(self, dim):\n",
    "        self.W1 = np.random.randn(dim, dim * 2) * 0.01\n",
    "        self.b1 = np.zeros(dim * 2)\n",
    "        self.W2 = np.random.randn(dim * 2, dim) * 0.01\n",
    "        self.b2 = np.zeros(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = np.maximum(0, x @ self.W1 + self.b1)\n",
    "        return h @ self.W2 + self.b2\n",
    "\n",
    "\n",
    "# =================== Mini BERT Encoder ====================\n",
    "class MiniBERTEncoder:\n",
    "    def __init__(self, vocab_size, dim, max_len):\n",
    "        self.embedding = np.random.randn(vocab_size, dim) * 0.01\n",
    "        self.pos_encoding = positional_encoding(max_len, dim)\n",
    "        self.mha = MiniMHA(dim)\n",
    "        self.ffn = MiniFFN(dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding[input_ids] + self.pos_encoding[:len(input_ids)]\n",
    "        attn_out = self.mha.forward(x)\n",
    "        x = norm(x + attn_out)\n",
    "        ffn_out = self.ffn.forward(x)\n",
    "        x = norm(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =================== NSP Head ====================\n",
    "class NSPHead:\n",
    "    def __init__(self, dim):\n",
    "        self.W = np.random.randn(dim, 2) * 0.01\n",
    "        self.b = np.zeros(2)\n",
    "\n",
    "    def forward(self, cls_vec):\n",
    "        logits = cls_vec @ self.W + self.b\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probs\n",
    "\n",
    "\n",
    "# =================== Full BERT NSP Model ====================\n",
    "class MiniBERTForNSP:\n",
    "    def __init__(self, vocab_size, dim, max_len):\n",
    "        self.encoder = MiniBERTEncoder(vocab_size, dim, max_len)\n",
    "        self.nsp = NSPHead(dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.encoder.forward(input_ids)\n",
    "        cls_vec = x[0]\n",
    "        return self.nsp.forward(cls_vec)\n",
    "\n",
    "\n",
    "# =================== Sample Sentences & Testing ====================\n",
    "sentences = [\n",
    "    \"Kucing tidur di atas sofa\",\n",
    "    \"Anjing bermain di taman\",\n",
    "    \"Langit berwarna biru hari ini\",\n",
    "    \"Dia sedang belajar matematika\"\n",
    "]\n",
    "\n",
    "build_vocab(sentences)\n",
    "\n",
    "# Pasangan yang benar (IsNext)\n",
    "s1 = \"Kucing tidur di atas sofa\"\n",
    "s2 = \"Langit berwarna biru hari ini\"\n",
    "\n",
    "# Pasangan acak (NotNext)\n",
    "s3 = \"Kucing tidur di atas sofa\"\n",
    "s4 = \"Dia sedang belajar matematika\"\n",
    "\n",
    "input_ids_true = encode(s1, s2)\n",
    "input_ids_false = encode(s3, s4)\n",
    "\n",
    "model = MiniBERTForNSP(vocab_size=len(vocab), dim=embedding_dim, max_len=max_len)\n",
    "\n",
    "print(\"Input TRUE:\", s1, \" + \", s2)\n",
    "print(\"NSP Output Prob (IsNext vs NotNext):\", model.forward(input_ids_true))\n",
    "\n",
    "print(\"\\nInput FALSE:\", s3, \" + \", s4)\n",
    "print(\"NSP Output Prob (IsNext vs NotNext):\", model.forward(input_ids_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for s1, s2, _ in dataset:\n",
    "    tokens.update(s1.lower().split())\n",
    "    tokens.update(s2.lower().split())\n",
    "\n",
    "tokens = sorted(list(tokens))\n",
    "vocab = {w: i+2 for i, w in enumerate(tokens)}  # 0 = PAD, 1 = CLS\n",
    "vocab[\"[PAD]\"] = 0\n",
    "vocab[\"[CLS]\"] = 1\n",
    "vocab[\"[SEP]\"] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Aku pergi ke toko\".lower().split()\n",
    "s2 = \"Lalu aku membeli roti\".lower().split()\n",
    "\n",
    "input_ids = [vocab[\"[CLS]\"]] + [vocab[w] for w in s1] + [vocab[\"[SEP]\"]] + [vocab[w] for w in s2] + [vocab[\"[SEP]\"]]\n",
    "max_len = 16\n",
    "\n",
    "if len(input_ids) < max_len:\n",
    "    input_ids += [vocab[\"[PAD]\"]] * (max_len - len(input_ids))\n",
    "else:\n",
    "    input_ids = input_ids[:max_len]\n",
    "\n",
    "input_ids = np.array(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita pakai Mini-BERT yang sebelumnya sudah kita bangun\n",
    "hidden_states = mini_bert_forward(input_ids)\n",
    "\n",
    "# Ambil output vector dari token [CLS] (posisi pertama)\n",
    "cls_vector = hidden_states[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = cls_vector.shape[0]  # misal 32 atau 64, tergantung BERT\n",
    "\n",
    "W = np.random.randn(hidden_dim, 2) * 0.01\n",
    "b = np.zeros(2)\n",
    "\n",
    "logits = cls_vector @ W + b\n",
    "\n",
    "# Softmax\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "pred = np.argmax(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(d_model):\n",
    "    token_result = []\n",
    "    print()\n",
    "    print(f\"Token {i}: {x[i]}\")\n",
    "    for j in range(0, d_model, 2):\n",
    "        xx = x[i][j]\n",
    "        yy = x[i][j + 1]\n",
    "        print(f\"Before Pos : (x,y) ({j}, {j+1}): ({xx}, {yy})\")\n",
    "\n",
    "        m = j\n",
    "        wi = 1/10000**(2*i/d_model)\n",
    "        thetha = m*wi\n",
    "\n",
    "        xx_r = xx*np.cos(thetha) - yy*np.sin(thetha)\n",
    "        yy_r = yy*np.sin(thetha) + yy*np.cos(thetha)\n",
    "\n",
    "        print(f\"After Pos : (x,y) ({j}, {j+1}): ({xx_r}, {yy_r})\")\n",
    "\n",
    "        # Store the results\n",
    "        result[i][j] = xx_r\n",
    "        if j + 1 < d_model: \n",
    "            result[i][j + 1] = yy_r\n",
    "                \n",
    "        token_result.extend([xx_r, yy_r])\n",
    "        \n",
    "    print(f\"Token {i} with positional encoding: {result[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: (kalimat, label) â€” label 1 = positif, 0 = negatif\n",
    "data = [\n",
    "    (\"aku suka banget sama film ini\", 1),\n",
    "    (\"filmnya bener-bener membosankan\", 0),\n",
    "    (\"aktingnya luar biasa\", 1),\n",
    "    (\"ngantuk banget pas nonton\", 0),\n",
    "    (\"ceritanya bikin terharu\", 1),\n",
    "    (\"gak masuk akal dan jelek\", 0),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f\"\\n[Batch {b}]\")\n",
    "    sentence_ids = input[b]\n",
    "    sentence_words = [id2w.get(i, \"[UNK]\") for i in sentence_ids]\n",
    "    print(\"Kalimat:\", \" \".join(sentence_words))\n",
    "    print(\"Token IDs:\", sentence_ids)\n",
    "    \n",
    "    for h in range(num_heads):\n",
    "        print(f\"\\n  Head {h}:\")\n",
    "        for q in range(seq_len):\n",
    "            q_word = id2w.get(sentence_ids[q], \"[UNK]\")\n",
    "            print(f\"    Query Token {q:2d} [{q_word:<10}]\")\n",
    "\n",
    "            for k in range(seq_len):\n",
    "                k_word = id2w.get(sentence_ids[k], \"[UNK]\")\n",
    "                score = scores[b, h, q, k]\n",
    "                weight = att_weights[b, h, q, k]\n",
    "                print(f\"      â†³ Key Token {k:2d} [{k_word:<10}] | Score: {score:>7.4f} | Softmax: {weight:>7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 contoh kalimat + label (1 = positif, 0 = negatif)\n",
    "inputs = np.array([\n",
    "    [1, 2, 3, 4, 5, 0, 0, 0, 0, 0],  # \"aku suka banget film ini\"\n",
    "    [1, 6, 7, 8, 9, 0, 0, 0, 0, 0]   # \"aku benci banget endingnya\"\n",
    "])\n",
    "labels = np.array([1, 0])\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "vocab_size = 20\n",
    "embedding_dim = 8\n",
    "head_dim = 4\n",
    "num_heads = 2\n",
    "ff_hidden = 32\n",
    "max_len = 10\n",
    "\n",
    "# Word Embedding\n",
    "W_embed = np.random.randn(vocab_size, embedding_dim)\n",
    "\n",
    "# Positional Encoding\n",
    "pos_embed = np.random.randn(max_len, embedding_dim)\n",
    "\n",
    "# MHA Proj\n",
    "W_q = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_k = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_v = np.random.randn(num_heads, embedding_dim, head_dim)\n",
    "W_o = np.random.randn(num_heads * head_dim, embedding_dim)\n",
    "\n",
    "# FFN\n",
    "W1 = np.random.randn(embedding_dim, ff_hidden)\n",
    "b1 = np.zeros(ff_hidden)\n",
    "W2 = np.random.randn(ff_hidden, embedding_dim)\n",
    "b2 = np.zeros(embedding_dim)\n",
    "\n",
    "# Classifier\n",
    "W_cls = np.random.randn(embedding_dim, 1)\n",
    "b_cls = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Sentiment Analysis\n",
    "\n",
    "1. data -> sentence and label\n",
    "2. vocab : add cls and pad in index 1 and 0\n",
    "3. tokenize : tokens the sentence and add cls and pad\n",
    "4. input numeric and labels : sentence, token numeric, dim token and labels\n",
    "5. embedding : seq_len, dim_token\n",
    "6. positional encoding : seq_len, dim_token\n",
    "7. pe = positional encoding + embedding\n",
    "8. multihead attention : batch_size, seq_len, dim_token, num_heads, head_dim\n",
    "9. feed forward : batch_size, seq_len, dim_token, dim_ffn\n",
    "10. logits : batch_size, seq_len, num_classes\n",
    "11. loss : cross entropy\n",
    "12. update parameter : gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Sample Data (Sentences and Labels)\n",
    "# In practice, you would load your own dataset\n",
    "class SentimentDataset:\n",
    "    def __init__(self):\n",
    "        self.sentences = [\n",
    "            \"I love this movie so much!\",\n",
    "            \"This was a terrible waste of time.\",\n",
    "            \"The acting was quite good in this film.\",\n",
    "            \"I would never recommend watching this.\",\n",
    "            \"An absolute masterpiece of cinema.\",\n",
    "            \"The plot makes no sense at all.\",\n",
    "            \"Such a heartwarming story throughout.\",\n",
    "            \"I fell asleep halfway through this boring movie.\",\n",
    "            \"The director did an outstanding job.\",\n",
    "            \"The characters were completely unrealistic.\"\n",
    "        ]\n",
    "        # Labels: 0 = negative, 1 = positive\n",
    "        self.labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.sentences, self.labels\n",
    "\n",
    "\n",
    "# 2. Vocabulary Creation\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize with special tokens\n",
    "        self.token2idx = {\"[PAD]\": 0, \"[CLS]\": 1, \"[UNK]\": 2}\n",
    "        self.idx2token = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[UNK]\"}\n",
    "        self.vocab_size = 3  # Start with special tokens\n",
    "    \n",
    "    def build_vocab(self, sentences: List[str], min_freq=1):\n",
    "        \"\"\"Build vocabulary from the sentences\"\"\"\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for sentence in sentences:\n",
    "            # Simple tokenization by splitting on spaces\n",
    "            for word in sentence.lower().split():\n",
    "                # Remove punctuation (simple version)\n",
    "                word = ''.join(c for c in word if c.isalnum())\n",
    "                if word:\n",
    "                    if word in word_freq:\n",
    "                        word_freq[word] += 1\n",
    "                    else:\n",
    "                        word_freq[word] = 1\n",
    "        \n",
    "        # Add words that meet minimum frequency\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq and word not in self.token2idx:\n",
    "                self.token2idx[word] = self.vocab_size\n",
    "                self.idx2token[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "        print(f\"Vocabulary built with {self.vocab_size} tokens\")\n",
    "    \n",
    "    def tokenize(self, sentence: str, max_len=128) -> List[int]:\n",
    "        \"\"\"Tokenize a sentence and return token indices\"\"\"\n",
    "        # Simple tokenization\n",
    "        words = sentence.lower().split()\n",
    "        words = [''.join(c for c in word if c.isalnum()) for word in words]\n",
    "        words = [word for word in words if word]\n",
    "        \n",
    "        # Convert to indices with [CLS] at the beginning\n",
    "        token_ids = [self.token2idx[\"[CLS]\"]]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.token2idx:\n",
    "                token_ids.append(self.token2idx[word])\n",
    "            else:\n",
    "                token_ids.append(self.token2idx[\"[UNK]\"])\n",
    "        \n",
    "        # Truncate or pad to max_len\n",
    "        if len(token_ids) > max_len:\n",
    "            token_ids = token_ids[:max_len]\n",
    "        else:\n",
    "            token_ids += [self.token2idx[\"[PAD]\"]] * (max_len - len(token_ids))\n",
    "        \n",
    "        return token_ids\n",
    "\n",
    "\n",
    "# 3. Sentiment Dataset Class for Training\n",
    "class SentimentDatasetLoader(Dataset):\n",
    "    def __init__(self, sentences, labels, vocab, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize sentence\n",
    "        token_ids = self.vocab.tokenize(sentence, self.max_len)\n",
    "        \n",
    "        # Create attention mask (1 for tokens, 0 for padding)\n",
    "        attention_mask = [1 if id != self.vocab.token2idx[\"[PAD]\"] else 0 for id in token_ids]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# 4. Model Components\n",
    "\n",
    "# Embedding Layer\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.embed_size)\n",
    "\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register buffer (won't be updated during backprop)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, embed_size]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "        self.out_proj = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Expand mask for multi-head attention\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax and get weighted sum\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Reshape back and apply final projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
    "        output = self.out_proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Feed Forward Layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, ff_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(embed_size, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "# Layer Normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_size, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(embed_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(embed_size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_size, num_heads)\n",
    "        self.feed_forward = FeedForward(embed_size, ff_dim)\n",
    "        self.norm1 = LayerNorm(embed_size)\n",
    "        self.norm2 = LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and normalization\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and normalization\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 5. Full BERT Model for Sentiment Analysis\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=768, num_heads=12, ff_dim=3072, \n",
    "                 num_layers=6, max_len=128, num_classes=2):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        # Token embedding and positional encoding\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
    "        self.position_encoding = PositionalEncoding(embed_size, max_len)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_size, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(embed_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Get embeddings with positional encoding\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.position_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        \n",
    "        # Classification using the [CLS] token (first token)\n",
    "        cls_token = x[:, 0, :]\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# 6. Training Functions\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# 7. Main Training Pipeline\n",
    "def main():\n",
    "    # Check for GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data = SentimentDataset()\n",
    "    sentences, labels = data.get_data()\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab(sentences)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SentimentDatasetLoader(sentences, labels, vocab)\n",
    "    \n",
    "    # Split into train and validation sets (80-20 split)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BERT(\n",
    "        vocab_size=vocab.vocab_size,\n",
    "        embed_size=256,     # Smaller than original BERT for demonstration\n",
    "        num_heads=8,\n",
    "        ff_dim=512,\n",
    "        num_layers=2,       # Fewer layers for faster training\n",
    "        max_len=128,\n",
    "        num_classes=2       # Binary sentiment classification\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"bert_sentiment.pt\")\n",
    "    print(\"Model saved to bert_sentiment.pt\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss History')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy History')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    print(\"Training history plot saved to training_history.png\")\n",
    "\n",
    "\n",
    "# 8. Test the model on new sentences\n",
    "def predict_sentiment(model, vocab, sentence, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    tokens = vocab.tokenize(sentence)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    attention_mask = torch.tensor([[1 if id != vocab.token2idx[\"[PAD]\"] else 0 for id in tokens]]).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "    \n",
    "    return \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the training pipeline\n",
    "    main()\n",
    "    \n",
    "    # Optional: Demonstrate prediction\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model and load weights\n",
    "    data = SentimentDataset()\n",
    "    sentences, _ = data.get_data()\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab(sentences)\n",
    "    \n",
    "    model = BERT(\n",
    "        vocab_size=vocab.vocab_size,\n",
    "        embed_size=256,\n",
    "        num_heads=8,\n",
    "        ff_dim=512,\n",
    "        num_layers=2,\n",
    "        max_len=128,\n",
    "        num_classes=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the saved model (if available)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"bert_sentiment.pt\"))\n",
    "        print(\"Loaded saved model.\")\n",
    "        \n",
    "        # Test on new sentences\n",
    "        test_sentences = [\n",
    "            \"This movie was amazing, I loved it!\",\n",
    "            \"I really hated the ending of this show.\",\n",
    "            \"The film was neither good nor bad, just average.\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTesting on new sentences:\")\n",
    "        for sentence in test_sentences:\n",
    "            sentiment = predict_sentiment(model, vocab, sentence, device)\n",
    "            print(f\"Sentence: '{sentence}'\")\n",
    "            print(f\"Predicted sentiment: {sentiment}\\n\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"No saved model found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA\n",
    "\n",
    "# sentences -> label 1 = positif, 0 = negatif\n",
    "sentences = [\n",
    "    (\"aku suka banget sama film ini\", 1),\n",
    "    (\"filmnya bener-bener membosankan\", 0),\n",
    "    (\"aktingnya luar biasa\", 1),\n",
    "    (\"ngantuk banget pas nonton\", 0),\n",
    "    (\"ceritanya bikin terharu\", 1),\n",
    "    (\"gak masuk akal dan jelek\", 0),\n",
    "]\n",
    "\n",
    "# vocab\n",
    "vocab = {}\n",
    "vocab['[CLS]'] = 1\n",
    "vocab['[PAD]'] = 0\n",
    "\n",
    "idx = 2\n",
    "\n",
    "for sentence, labels in sentences:\n",
    "    for word in sentence.split():\n",
    "        vocab[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "## word to index and index to word\n",
    "w2id = {word : idx for idx, word in enumerate(vocab)}\n",
    "id2w = {idx : word for word, idx in vocab.items()}\n",
    "\n",
    "## tokenize\n",
    "def tokenize(sentence, max_len=10):\n",
    "    tokens = sentence.lower().split()\n",
    "    ids = [vocab[\"[CLS]\"]] + [vocab.get(tok, 0) for tok in tokens]\n",
    "    # Padding\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "## input and label\n",
    "import numpy as np\n",
    "\n",
    "input = []\n",
    "lables = []\n",
    "\n",
    "for sentence, labels in sentences:\n",
    "    ids = tokenize(sentence)\n",
    "    input.append(ids)\n",
    "    lables.append(labels)\n",
    "\n",
    "input = np.array(input)\n",
    "labels = np.array(lables)\n",
    "\n",
    "## embedding layer\n",
    "dim = 4\n",
    "\n",
    "np.random.seed(4)\n",
    "embedding_matrix = np.random.randn(len(vocab)+1, dim)\n",
    "input_embedding = embedding_matrix[input]\n",
    "\n",
    "## positional encoding\n",
    "max_len = 10\n",
    "\n",
    "pos = np.arange(max_len)[:, np.newaxis]  # (10, 1)\n",
    "i = np.arange(dim)[np.newaxis, :]        # (1, 8)\n",
    "\n",
    "# Hitung angle rates\n",
    "angle_rates = 1 / np.power(10000, (2 * (i // 2)) / dim)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "\n",
    "# Hitung angle radians\n",
    "angle_rads = pos * angle_rates\n",
    "\n",
    "# Terapkan sin ke index genap dan cos ke index ganjil\n",
    "angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # even\n",
    "angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # odd\n",
    "\n",
    "pe = input_embedding + angle_rads[np.newaxis, :input_embedding.shape[1], :]\n",
    "\n",
    "## multihead self attention\n",
    "# Parameters\n",
    "batch_size, seq_len, embed_dim = pe.shape\n",
    "num_heads = 4\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"seq_len   :\", seq_len)\n",
    "print(\"embed_dim :\", embed_dim)\n",
    "print(\"num_heads :\", num_heads)\n",
    "print(\"head_dim  :\", head_dim)\n",
    "\n",
    "# Weights\n",
    "wq = np.random.randn(num_heads, embed_dim, head_dim)\n",
    "wk = np.random.randn(num_heads, embed_dim, head_dim)\n",
    "wv = np.random.randn(num_heads, embed_dim, head_dim)\n",
    "wo = np.random.randn(num_heads * head_dim, embed_dim)\n",
    "\n",
    "# Proyeksi Q, K, V\n",
    "# hasil: (batch, head, seq_len, head_dim)\n",
    "Q = np.einsum('bse,hed->bhsd', pe, wq)\n",
    "K = np.einsum('bse,hed->bhsd', pe, wk)\n",
    "V = np.einsum('bse,hed->bhsd', pe, wv)\n",
    "\n",
    "# Scaled dot-product attention\n",
    "# (b, h, s, s)\n",
    "scores = np.einsum('bhsd,bhtd->bhst', Q, K) / np.sqrt(head_dim)\n",
    "\n",
    "# Softmax\n",
    "scores -= np.max(scores, axis=-1, keepdims=True)\n",
    "att_weights = np.exp(scores)\n",
    "att_weights /= np.sum(att_weights, axis=-1, keepdims=True)\n",
    "\n",
    "# Attention output: (b, h, s, d)\n",
    "att_out = np.einsum('bhst,bhtd->bhsd', att_weights, V)\n",
    "\n",
    "# Concatenate heads: (b, s, h*d)\n",
    "att_concat = att_out.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "\n",
    "# Output projection: (b, s, embed_dim)\n",
    "att_projected = np.einsum('bsd,df->bsf', att_concat, wo)\n",
    "\n",
    "\n",
    "## NORM AND ADD\n",
    "def norm(x):\n",
    "    eps = 1e-6\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + eps)\n",
    "\n",
    "# Residual + norm\n",
    "add = pe + att_projected\n",
    "norm1 = norm(add)\n",
    "\n",
    "## FFN\n",
    "batch_size, seq_len, embed_dim = pe.shape\n",
    "ff_dim = 2 * embed_dim\n",
    "W1 = np.random.randn(embed_dim, ff_dim)\n",
    "b1 = np.zeros(ff_dim)\n",
    "W2 = np.random.randn(ff_dim, embed_dim)\n",
    "b2 = np.zeros(embed_dim)\n",
    "\n",
    "# Dense -> ReLU -> Dense\n",
    "\n",
    "expanded_input = norm1 @ W1 + b1\n",
    "\n",
    "relu = np.maximum(0, expanded_input) \n",
    "\n",
    "compressed_output = relu @ W2 + b2\n",
    "\n",
    "## NORM AND ADD\n",
    "add1 = norm1 + compressed_output\n",
    "norm2 = norm(add1)\n",
    "\n",
    "## CLASSIFICATION HEAD\n",
    "sentence_ids = input[b]\n",
    "sentence_words = [id2w.get(i, \"[UNK]\") for i in sentence_ids]\n",
    "true_label = labels[b]\n",
    "\n",
    "cls_output = norm2[:, 0, :]\n",
    "num_classes = 2\n",
    "W_cls = np.random.randn(embed_dim, num_classes)\n",
    "b_cls = np.zeros(num_classes)\n",
    "\n",
    "# Logits & Probabilities\n",
    "logit = cls_output @ W_cls + b_cls\n",
    "prob = softmax(logit)\n",
    "pred_label = np.argmax(prob)\n",
    "loss = cross_entropy(prob, true_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.0190\n",
      "Epoch 2, Loss: 4.0190\n",
      "Epoch 3, Loss: 4.0190\n",
      "Epoch 4, Loss: 4.0190\n",
      "Epoch 5, Loss: 4.0190\n",
      "Epoch 6, Loss: 4.0190\n",
      "Epoch 7, Loss: 4.0190\n",
      "Epoch 8, Loss: 4.0190\n",
      "Epoch 9, Loss: 4.0190\n",
      "Epoch 10, Loss: 4.0190\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 306\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Predict new sentences\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 277\u001b[0m, in \u001b[0;36mBERTSentimentClassifier.evaluate\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m    275\u001b[0m input_ids, true_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batch(test_data)\n\u001b[1;32m    276\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_ids)\n\u001b[0;32m--> 277\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(pred_labels \u001b[38;5;241m==\u001b[39m true_labels)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[0;32mIn[1], line 197\u001b[0m, in \u001b[0;36mClassificationHead.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 197\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(logits)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m, in \u001b[0;36mClassificationHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Use the [CLS] token output\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     cls_output \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# Calculate logits\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     logits \u001b[38;5;241m=\u001b[39m cls_output \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_cls \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_cls\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VocabProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            '[CLS]': 1,\n",
    "            '[PAD]': 0\n",
    "        }\n",
    "        self.w2id = None\n",
    "        self.id2w = None\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary from list of sentences\"\"\"\n",
    "        idx = 2  # Start after special tokens\n",
    "        \n",
    "        for sentence, _ in sentences:\n",
    "            for word in sentence.split():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = idx\n",
    "                    idx += 1\n",
    "        \n",
    "        # Create mappings\n",
    "        self.w2id = {word: idx for word, idx in self.vocab.items()}\n",
    "        self.id2w = {idx: word for word, idx in self.w2id.items()}\n",
    "        \n",
    "        return self.vocab\n",
    "    \n",
    "    def tokenize(self, sentence, max_len=10):\n",
    "        \"\"\"Convert sentence to token IDs with padding/truncation\"\"\"\n",
    "        tokens = sentence.lower().split()\n",
    "        ids = [self.vocab[\"[CLS]\"]] + [self.vocab.get(tok, 0) for tok in tokens]\n",
    "        \n",
    "        # Padding\n",
    "        if len(ids) < max_len:\n",
    "            ids += [self.vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        \n",
    "        return ids\n",
    "\n",
    "\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, embed_dim, seed=4):\n",
    "        np.random.seed(seed)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding_matrix = np.random.randn(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Convert token IDs to embeddings\"\"\"\n",
    "        return self.embedding_matrix[input_ids]\n",
    "\n",
    "\n",
    "class PositionalEncoder:\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pe = self._create_positional_encoding()\n",
    "    \n",
    "    def _create_positional_encoding(self):\n",
    "        pos = np.arange(self.max_len)[:, np.newaxis]  # (max_len, 1)\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]  # (1, embed_dim)\n",
    "        \n",
    "        # Calculate angle rates\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / self.embed_dim)\n",
    "        \n",
    "        # Calculate angle radians\n",
    "        angle_rads = pos * angle_rates\n",
    "        \n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return angle_rads\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"Add positional encoding to embeddings\"\"\"\n",
    "        batch_size, seq_len, _ = embeddings.shape\n",
    "        return embeddings + self.pe[:seq_len, :]\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.wq = np.random.randn(num_heads, embed_dim, self.head_dim)\n",
    "        self.wk = np.random.randn(num_heads, embed_dim, self.head_dim)\n",
    "        self.wv = np.random.randn(num_heads, embed_dim, self.head_dim)\n",
    "        self.wo = np.random.randn(num_heads * self.head_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = np.einsum('bse,hed->bhsd', x, self.wq)\n",
    "        K = np.einsum('bse,hed->bhsd', x, self.wk)\n",
    "        V = np.einsum('bse,hed->bhsd', x, self.wv)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = np.einsum('bhsd,bhtd->bhst', Q, K) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Softmax\n",
    "        scores -= np.max(scores, axis=-1, keepdims=True)\n",
    "        att_weights = np.exp(scores)\n",
    "        att_weights /= np.sum(att_weights, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Attention output\n",
    "        att_out = np.einsum('bhst,bhtd->bhsd', att_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        att_concat = att_out.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Output projection\n",
    "        att_projected = np.einsum('bsd,df->bsf', att_concat, self.wo)\n",
    "        \n",
    "        return att_projected\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True)\n",
    "        return (x - mean) / (std + self.eps)\n",
    "\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, embed_dim, ff_dim=None):\n",
    "        if ff_dim is None:\n",
    "            ff_dim = 2 * embed_dim\n",
    "            \n",
    "        self.W1 = np.random.randn(embed_dim, ff_dim)\n",
    "        self.b1 = np.zeros(ff_dim)\n",
    "        self.W2 = np.random.randn(ff_dim, embed_dim)\n",
    "        self.b2 = np.zeros(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First dense layer\n",
    "        expanded_input = x @ self.W1 + self.b1\n",
    "        \n",
    "        # ReLU activation\n",
    "        relu = np.maximum(0, expanded_input)\n",
    "        \n",
    "        # Second dense layer\n",
    "        return relu @ self.W2 + self.b2\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = LayerNorm()\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.norm2 = LayerNorm()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Multi-head attention\n",
    "        att_output = self.attention.forward(x)\n",
    "        \n",
    "        # Add & norm\n",
    "        add1 = x + att_output\n",
    "        norm1 = self.norm1.forward(add1)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward.forward(norm1)\n",
    "        \n",
    "        # Add & norm\n",
    "        add2 = norm1 + ff_output\n",
    "        norm2 = self.norm2.forward(add2)\n",
    "        \n",
    "        return norm2\n",
    "\n",
    "\n",
    "class ClassificationHead:\n",
    "    def __init__(self, embed_dim, num_classes=2):\n",
    "        self.W_cls = np.random.randn(embed_dim, num_classes)\n",
    "        self.b_cls = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Use the [CLS] token output\n",
    "        cls_output = x[:, 0, :]\n",
    "        \n",
    "        # Calculate logits\n",
    "        logits = cls_output @ self.W_cls + self.b_cls\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x, axis=-1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return np.argmax(probs, axis=-1)\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        # Get probability of the true class for each sample\n",
    "        true_probs = np.array([probs[i, labels[i]] for i in range(batch_size)])\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        return -np.log(true_probs + 1e-10)\n",
    "\n",
    "\n",
    "class BERTSentimentClassifier:\n",
    "    def __init__(self, max_len=10, embed_dim=4, num_heads=4):\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_processor = VocabProcessor()\n",
    "        self.embedding_layer = None  # Will be initialized after vocabulary is built\n",
    "        self.positional_encoder = PositionalEncoder(max_len, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads)\n",
    "        self.classifier = ClassificationHead(embed_dim)\n",
    "    \n",
    "    def initialize(self, sentences):\n",
    "        # Build vocabulary\n",
    "        self.vocab_processor.build_vocab(sentences)\n",
    "        \n",
    "        # Initialize embedding layer with the vocabulary size\n",
    "        vocab_size = len(self.vocab_processor.vocab) + 1  # +1 for handling unknown tokens\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, self.embed_dim)\n",
    "    \n",
    "    def _prepare_batch(self, sentences):\n",
    "        # Convert sentences to token IDs\n",
    "        input_ids = np.array([self.vocab_processor.tokenize(sentence) for sentence, _ in sentences])\n",
    "        labels = np.array([label for _, label in sentences])\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding_layer.forward(input_ids)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_embeddings = self.positional_encoder.forward(embeddings)\n",
    "        \n",
    "        # Transformer block\n",
    "        transformer_output = self.transformer.forward(pos_embeddings)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier.forward(transformer_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def train_step(self, batch, learning_rate=0.01):\n",
    "        # This is a simplified training step - in a real implementation, \n",
    "        # you would compute gradients and update weights\n",
    "        input_ids, labels = self._prepare_batch(batch)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.classifier.cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Here you would add backpropagation and parameter updates\n",
    "        # For demonstration purposes, we're returning the loss\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def predict(self, sentences):\n",
    "        # For single sentence or list of sentence strings\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [(sentences, 0)]  # Dummy label\n",
    "        elif isinstance(sentences[0], str):\n",
    "            sentences = [(sent, 0) for sent in sentences]  # Dummy labels\n",
    "            \n",
    "        input_ids, _ = self._prepare_batch(sentences)\n",
    "        logits = self.forward(input_ids)\n",
    "        return self.classifier.predict(logits)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        input_ids, true_labels = self._prepare_batch(test_data)\n",
    "        logits = self.forward(input_ids)\n",
    "        pred_labels = self.classifier.predict(logits)\n",
    "        \n",
    "        accuracy = np.mean(pred_labels == true_labels)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    sentences = [\n",
    "        (\"aku suka banget sama film ini\", 1),\n",
    "        (\"filmnya bener-bener membosankan\", 0),\n",
    "        (\"aktingnya luar biasa\", 1),\n",
    "        (\"ngantuk banget pas nonton\", 0),\n",
    "        (\"ceritanya bikin terharu\", 1),\n",
    "        (\"gak masuk akal dan jelek\", 0),\n",
    "    ]\n",
    "    \n",
    "    # Create and initialize the model\n",
    "    model = BERTSentimentClassifier(max_len=10, embed_dim=4, num_heads=4)\n",
    "    model.initialize(sentences)\n",
    "    \n",
    "    # Training loop (simplified)\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = model.train_step(sentences)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = model.evaluate(sentences)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Predict new sentences\n",
    "    new_sentences = [\n",
    "        (\"film ini sangat bagus\", 0),  # Should predict 1 (positive)\n",
    "        (\"saya kecewa dengan ceritanya\", 0)  # Should predict 0 (negative)\n",
    "    ]\n",
    "    \n",
    "    predictions = model.predict(new_sentences)\n",
    "    for i, (sentence, _) in enumerate(new_sentences):\n",
    "        sentiment = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
    "        print(f\"Sentence: '{sentence}', Predicted: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BERTSentimentClassifier.train_step() got an unexpected keyword argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 690\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# Training loop with proper backpropagation and parameter updates\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 690\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    693\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(sentences)\n",
      "\u001b[0;31mTypeError\u001b[0m: BERTSentimentClassifier.train_step() got an unexpected keyword argument 'epochs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BERTSentimentClassifier:\n",
    "    def __init__(self, max_len=10, embed_dim=4, num_heads=4):\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_processor = VocabProcessor()\n",
    "        self.embedding_layer = None  # Will be initialized after vocabulary is built\n",
    "        self.positional_encoder = PositionalEncoder(max_len, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads)\n",
    "        self.classifier = ClassificationHead(embed_dim)\n",
    "    \n",
    "    def initialize(self, sentences):\n",
    "        # Build vocabulary\n",
    "        self.vocab_processor.build_vocab(sentences)\n",
    "        \n",
    "        # Initialize embedding layer with the vocabulary size\n",
    "        vocab_size = len(self.vocab_processor.vocab) + 1  # +1 for handling unknown tokens\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, self.embed_dim)\n",
    "    \n",
    "    def _prepare_batch(self, sentences):\n",
    "        # Convert sentences to token IDs\n",
    "        input_ids = np.array([self.vocab_processor.tokenize(sentence) for sentence, _ in sentences])\n",
    "        labels = np.array([label for _, label in sentences])\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def forward(self, input_ids, cache=True):\n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding_layer.forward(input_ids, cache)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        batch_size, seq_len, _ = embeddings.shape\n",
    "        pos_embeddings = self.positional_encoder.forward(embeddings)\n",
    "        \n",
    "        # Transformer block\n",
    "        transformer_output = self.transformer.forward(pos_embeddings, cache)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier.forward(transformer_output, cache)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def backward(self, logits, labels):\n",
    "        # Loss and gradients through classifier\n",
    "        loss = np.mean(self.classifier.cross_entropy_loss(logits, labels))\n",
    "        \n",
    "        # Backpropagate through classifier\n",
    "        dW_cls, db_cls, dcls_output = self.classifier.backward()\n",
    "        \n",
    "        # Create gradient for transformer output\n",
    "        # Only the [CLS] token is used for classification\n",
    "        dtransformer_output = np.zeros((dcls_output.shape[0], self.max_len, self.embed_dim))\n",
    "        dtransformer_output[:, 0, :] = dcls_output  # Only pass gradient to [CLS] token\n",
    "        \n",
    "        # Backpropagate through transformer\n",
    "        transformer_grads = self.transformer.backward(dtransformer_output)\n",
    "        \n",
    "        # Backpropagate through embedding layer\n",
    "        dembedding_matrix = self.embedding_layer.backward(transformer_grads['dx'])\n",
    "        \n",
    "        # Collect all gradients to return\n",
    "        gradients = {\n",
    "            'dW_cls': dW_cls,\n",
    "            'db_cls': db_cls,\n",
    "            'dembedding_matrix': dembedding_matrix,\n",
    "            'transformer': transformer_grads\n",
    "        }\n",
    "        \n",
    "        return loss, gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        # Update classifier weights\n",
    "        self.classifier.W_cls -= learning_rate * gradients['dW_cls']\n",
    "        self.classifier.b_cls -= learning_rate * gradients['db_cls']\n",
    "        \n",
    "        # Update embedding matrix\n",
    "        self.embedding_layer.embedding_matrix -= learning_rate * gradients['dembedding_matrix']\n",
    "        \n",
    "        # Update transformer parameters\n",
    "        # Update attention weights\n",
    "        self.transformer.attention.wq -= learning_rate * gradients['transformer']['wq']\n",
    "        self.transformer.attention.wk -= learning_rate * gradients['transformer']['wk']\n",
    "        self.transformer.attention.wv -= learning_rate * gradients['transformer']['wv']\n",
    "        self.transformer.attention.wo -= learning_rate * gradients['transformer']['wo']\n",
    "        \n",
    "        # Update feed-forward weights\n",
    "        self.transformer.feed_forward.W1 -= learning_rate * gradients['transformer']['W1']\n",
    "        self.transformer.feed_forward.b1 -= learning_rate * gradients['transformer']['b1']\n",
    "        self.transformer.feed_forward.W2 -= learning_rate * gradients['transformer']['W2']\n",
    "        self.transformer.feed_forward.b2 -= learning_rate * gradients['transformer']['b2']\n",
    "    \n",
    "    def train_step(self, batch, learning_rate=0.01):\n",
    "        # Prepare batch data\n",
    "        input_ids, labels = self._prepare_batch(batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.forward(input_ids)\n",
    "        \n",
    "        # Backward pass to compute gradients\n",
    "        loss, gradients = self.backward(logits, labels)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters(gradients, learning_rate)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, data, epochs=10, learning_rate=0.01, batch_size=None):\n",
    "        \"\"\"Train the model for multiple epochs\"\"\"\n",
    "        # If batch_size is None, use the entire dataset as one batch\n",
    "        if batch_size is None:\n",
    "            batch_size = len(data)\n",
    "        \n",
    "        # Training history\n",
    "        history = {'loss': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            np.random.shuffle(data)\n",
    "            \n",
    "            # Initialize epoch loss\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Process batches\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                batch_loss = self.train_step(batch, learning_rate)\n",
    "                epoch_loss += batch_loss\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_loss = epoch_loss / (len(data) / batch_size)\n",
    "            history['loss'].append(avg_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, sentences):\n",
    "        # For single sentence or list of sentence strings\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [(sentences, 0)]  # Dummy label\n",
    "        elif isinstance(sentences[0], str):\n",
    "            sentences = [(sent, 0) for sent in sentences]  # Dummy labels\n",
    "            \n",
    "        input_ids, _ = self._prepare_batch(sentences)\n",
    "        logits = self.forward(input_ids, cache=False)  # No need to cache during inference\n",
    "        return self.classifier.predict(logits)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        input_ids, true_labels = self._prepare_batch(test_data)\n",
    "        logits = self.forward(input_ids, cache=False)\n",
    "        pred_labels = self.classifier.predict(logits)\n",
    "        \n",
    "        accuracy = np.mean(pred_labels == true_labels)\n",
    "        return accuracy\n",
    "\n",
    "class VocabProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            '[CLS]': 1,\n",
    "            '[PAD]': 0\n",
    "        }\n",
    "        self.w2id = None\n",
    "        self.id2w = None\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary from list of sentences\"\"\"\n",
    "        idx = 2  # Start after special tokens\n",
    "        \n",
    "        for sentence, _ in sentences:\n",
    "            for word in sentence.split():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = idx\n",
    "                    idx += 1\n",
    "        \n",
    "        # Create mappings\n",
    "        self.w2id = {word: idx for word, idx in self.vocab.items()}\n",
    "        self.id2w = {idx: word for word, idx in self.w2id.items()}\n",
    "        \n",
    "        return self.vocab\n",
    "    \n",
    "    def tokenize(self, sentence, max_len=10):\n",
    "        \"\"\"Convert sentence to token IDs with padding/truncation\"\"\"\n",
    "        tokens = sentence.lower().split()\n",
    "        ids = [self.vocab[\"[CLS]\"]] + [self.vocab.get(tok, 0) for tok in tokens]\n",
    "        \n",
    "        # Padding\n",
    "        if len(ids) < max_len:\n",
    "            ids += [self.vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        \n",
    "        return ids\n",
    "\n",
    "\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, embed_dim, seed=4):\n",
    "        np.random.seed(seed)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_matrix = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, input_ids, cache=True):\n",
    "        \"\"\"Convert token IDs to embeddings\"\"\"\n",
    "        embeddings = self.embedding_matrix[input_ids]\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['input_ids'] = input_ids\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def backward(self, dembeddings):\n",
    "        \"\"\"Backpropagate gradient to embedding matrix\"\"\"\n",
    "        input_ids = self.cache['input_ids']\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Initialize gradients for embedding matrix\n",
    "        dembedding_matrix = np.zeros_like(self.embedding_matrix)\n",
    "        \n",
    "        # For each position in the batch, accumulate gradients for the corresponding word vector\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                word_idx = input_ids[b, s]\n",
    "                dembedding_matrix[word_idx] += dembeddings[b, s]\n",
    "        \n",
    "        return dembedding_matrix\n",
    "\n",
    "\n",
    "class PositionalEncoder:\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pe = self._create_positional_encoding()\n",
    "    \n",
    "    def _create_positional_encoding(self):\n",
    "        pos = np.arange(self.max_len)[:, np.newaxis]  # (max_len, 1)\n",
    "        i = np.arange(self.embed_dim)[np.newaxis, :]  # (1, embed_dim)\n",
    "        \n",
    "        # Calculate angle rates\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / self.embed_dim)\n",
    "        \n",
    "        # Calculate angle radians\n",
    "        angle_rads = pos * angle_rates\n",
    "        \n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return angle_rads\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"Add positional encoding to embeddings\"\"\"\n",
    "        batch_size, seq_len, _ = embeddings.shape\n",
    "        return embeddings + self.pe[:seq_len, :]\n",
    "    \n",
    "    # No need for backward method as positional encodings are fixed\n",
    "\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.wq = np.random.randn(num_heads, embed_dim, self.head_dim) * 0.01\n",
    "        self.wk = np.random.randn(num_heads, embed_dim, self.head_dim) * 0.01\n",
    "        self.wv = np.random.randn(num_heads, embed_dim, self.head_dim) * 0.01\n",
    "        self.wo = np.random.randn(num_heads * self.head_dim, embed_dim) * 0.01\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = np.einsum('bse,hed->bhsd', x, self.wq)\n",
    "        K = np.einsum('bse,hed->bhsd', x, self.wk)\n",
    "        V = np.einsum('bse,hed->bhsd', x, self.wv)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = np.einsum('bhsd,bhtd->bhst', Q, K) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Softmax\n",
    "        scores_max = np.max(scores, axis=-1, keepdims=True)\n",
    "        exp_scores = np.exp(scores - scores_max)\n",
    "        att_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Attention output\n",
    "        att_out = np.einsum('bhst,bhtd->bhsd', att_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        att_concat = att_out.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Output projection\n",
    "        att_projected = np.einsum('bsd,df->bsf', att_concat, self.wo)\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['Q'] = Q\n",
    "            self.cache['K'] = K\n",
    "            self.cache['V'] = V\n",
    "            self.cache['scores'] = scores\n",
    "            self.cache['att_weights'] = att_weights\n",
    "            self.cache['att_out'] = att_out\n",
    "            self.cache['att_concat'] = att_concat\n",
    "        \n",
    "        return att_projected\n",
    "    \n",
    "    def backward(self, datt_projected):\n",
    "        # Retrieve from cache\n",
    "        x = self.cache['x']\n",
    "        Q = self.cache['Q']\n",
    "        K = self.cache['K']\n",
    "        V = self.cache['V']\n",
    "        att_weights = self.cache['att_weights']\n",
    "        att_out = self.cache['att_out']\n",
    "        att_concat = self.cache['att_concat']\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Gradient w.r.t output projection (wo)\n",
    "        dwo = np.einsum('bsd,bsf->df', att_concat, datt_projected)\n",
    "        \n",
    "        # Gradient w.r.t concatenated attention\n",
    "        datt_concat = np.einsum('bsf,df->bsd', datt_projected, self.wo)\n",
    "        \n",
    "        # Reshape back to multi-head format\n",
    "        datt_out = datt_concat.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # Gradient w.r.t V\n",
    "        dwv = np.einsum('bhst,bhsd->hed', att_weights, datt_out)\n",
    "        \n",
    "        # Gradient w.r.t attention weights\n",
    "        datt_weights = np.einsum('bhsd,bhtd->bhst', datt_out, V)\n",
    "        \n",
    "        # Gradient through softmax\n",
    "        dscores = att_weights * (datt_weights - np.sum(att_weights * datt_weights, axis=-1, keepdims=True))\n",
    "        \n",
    "        # Gradient w.r.t Q and K\n",
    "        dwq = np.einsum('bhst,bse->hed', dscores / np.sqrt(self.head_dim), x)\n",
    "        dwk = np.einsum('bhst,bte->hed', dscores.transpose(0, 1, 3, 2) / np.sqrt(self.head_dim), x)\n",
    "        \n",
    "        # Gradient w.r.t input x (for Q)\n",
    "        dx_q = np.einsum('bhsd,hed->bse', dscores / np.sqrt(self.head_dim), self.wq)\n",
    "        \n",
    "        # Gradient w.r.t input x (for K)\n",
    "        dx_k = np.einsum('bhts,hed->bte', dscores / np.sqrt(self.head_dim), self.wk)\n",
    "        \n",
    "        # Gradient w.r.t input x (for V)\n",
    "        dx_v = np.einsum('bhst,hed->bte', att_weights, dwv)\n",
    "        \n",
    "        # Combine gradients from Q, K, and V paths\n",
    "        dx = dx_q + dx_k + dx_v\n",
    "        \n",
    "        return dwq, dwk, dwv, dwo, dx\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        normalized = (x - mean) / std\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['mean'] = mean\n",
    "            self.cache['var'] = var\n",
    "            self.cache['std'] = std\n",
    "            self.cache['normalized'] = normalized\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Retrieve from cache\n",
    "        x = self.cache['x']\n",
    "        mean = self.cache['mean']\n",
    "        var = self.cache['var']\n",
    "        std = self.cache['std']\n",
    "        \n",
    "        N = x.shape[-1]\n",
    "        \n",
    "        # Gradient calculations for layer normalization\n",
    "        # Based on derivation of layer norm backward pass\n",
    "        dx_normalized = dout\n",
    "        \n",
    "        # Gradient w.r.t variance\n",
    "        dvar = -0.5 * np.sum(dx_normalized * (x - mean) * np.power(var + self.eps, -1.5), axis=-1, keepdims=True)\n",
    "        \n",
    "        # Gradient w.r.t mean\n",
    "        dmean = -np.sum(dx_normalized / std, axis=-1, keepdims=True) + dvar * np.mean(-2.0 * (x - mean), axis=-1, keepdims=True)\n",
    "        \n",
    "        # Gradient w.r.t input\n",
    "        dx = dx_normalized / std + dvar * 2.0 * (x - mean) / N + dmean / N\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, embed_dim, ff_dim=None):\n",
    "        if ff_dim is None:\n",
    "            ff_dim = 2 * embed_dim\n",
    "            \n",
    "        self.W1 = np.random.randn(embed_dim, ff_dim) * 0.01\n",
    "        self.b1 = np.zeros(ff_dim)\n",
    "        self.W2 = np.random.randn(ff_dim, embed_dim) * 0.01\n",
    "        self.b2 = np.zeros(embed_dim)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        # First dense layer\n",
    "        expanded_input = x @ self.W1 + self.b1\n",
    "        \n",
    "        # ReLU activation\n",
    "        relu = np.maximum(0, expanded_input)\n",
    "        \n",
    "        # Second dense layer\n",
    "        output = relu @ self.W2 + self.b2\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['input'] = x\n",
    "            self.cache['expanded_input'] = expanded_input\n",
    "            self.cache['relu'] = relu\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        # Retrieve from cache\n",
    "        x = self.cache['input']\n",
    "        expanded_input = self.cache['expanded_input']\n",
    "        relu = self.cache['relu']\n",
    "        \n",
    "        # Gradient w.r.t W2 and b2\n",
    "        dW2 = relu.T @ doutput\n",
    "        db2 = np.sum(doutput, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t relu\n",
    "        drelu = doutput @ self.W2.T\n",
    "        \n",
    "        # Gradient through ReLU (ReLU gradient is 1 where input > 0, 0 otherwise)\n",
    "        dexpanded_input = drelu * (expanded_input > 0)\n",
    "        \n",
    "        # Gradient w.r.t W1 and b1\n",
    "        dW1 = x.T @ dexpanded_input\n",
    "        db1 = np.sum(dexpanded_input, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t input\n",
    "        dx = dexpanded_input @ self.W1.T\n",
    "        \n",
    "        return dW1, db1, dW2, db2, dx\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = LayerNorm()\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.norm2 = LayerNorm()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        # Multi-head attention\n",
    "        att_output = self.attention.forward(x, cache)\n",
    "        \n",
    "        # Add & norm\n",
    "        add1 = x + att_output\n",
    "        norm1 = self.norm1.forward(add1, cache)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ff_output = self.feed_forward.forward(norm1, cache)\n",
    "        \n",
    "        # Add & norm\n",
    "        add2 = norm1 + ff_output\n",
    "        norm2 = self.norm2.forward(add2, cache)\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['att_output'] = att_output\n",
    "            self.cache['add1'] = add1\n",
    "            self.cache['norm1'] = norm1\n",
    "            self.cache['ff_output'] = ff_output\n",
    "            self.cache['add2'] = add2\n",
    "        \n",
    "        return norm2\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Retrieve from cache\n",
    "        x = self.cache['x']\n",
    "        att_output = self.cache['att_output']\n",
    "        add1 = self.cache['add1']\n",
    "        norm1 = self.cache['norm1']\n",
    "        ff_output = self.cache['ff_output']\n",
    "        \n",
    "        # Backward through norm2\n",
    "        dadd2 = self.norm2.backward(dout)\n",
    "        \n",
    "        # Gradient split at residual connection\n",
    "        dnorm1 = dadd2\n",
    "        dff_output = dadd2\n",
    "        \n",
    "        # Backward through feed-forward\n",
    "        dW1, db1, dW2, db2, dnorm1_ff = self.feed_forward.backward(dff_output)\n",
    "        \n",
    "        # Add gradients from residual connection and FF path\n",
    "        dnorm1 += dnorm1_ff\n",
    "        \n",
    "        # Backward through norm1\n",
    "        dadd1 = self.norm1.backward(dnorm1)\n",
    "        \n",
    "        # Gradient split at residual connection\n",
    "        dx = dadd1\n",
    "        datt_output = dadd1\n",
    "        \n",
    "        # Backward through attention\n",
    "        dwq, dwk, dwv, dwo, dx_att = self.attention.backward(datt_output)\n",
    "        \n",
    "        # Add gradients from residual connection and attention path\n",
    "        dx += dx_att\n",
    "        \n",
    "        return {\n",
    "            'wq': dwq, 'wk': dwk, 'wv': dwv, 'wo': dwo,\n",
    "            'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2,\n",
    "            'dx': dx\n",
    "        }\n",
    "\n",
    "\n",
    "class ClassificationHead:\n",
    "    def __init__(self, embed_dim, num_classes=2):\n",
    "        self.W_cls = np.random.randn(embed_dim, num_classes) * 0.01  # Initialize with small values\n",
    "        self.b_cls = np.zeros(num_classes)\n",
    "        self.cache = {}  # For storing intermediates for backpropagation\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        # Use the [CLS] token output\n",
    "        cls_output = x[:, 0, :]\n",
    "        \n",
    "        # Calculate logits\n",
    "        logits = cls_output @ self.W_cls + self.b_cls\n",
    "        \n",
    "        if cache:\n",
    "            self.cache['cls_output'] = cls_output\n",
    "            self.cache['logits'] = logits\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        probs = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x, cache=False)\n",
    "        probs = self.softmax(logits)\n",
    "        return np.argmax(probs, axis=-1)\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        probs = self.softmax(logits)\n",
    "        \n",
    "        # Store for backpropagation\n",
    "        self.cache['probs'] = probs\n",
    "        self.cache['labels'] = labels\n",
    "        \n",
    "        # Get probability of the true class for each sample\n",
    "        true_probs = np.array([probs[i, labels[i]] for i in range(batch_size)])\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        return -np.log(true_probs + 1e-10)\n",
    "    \n",
    "    def backward(self):\n",
    "        # Retrieve from cache\n",
    "        probs = self.cache['probs']\n",
    "        labels = self.cache['labels']\n",
    "        cls_output = self.cache['cls_output']\n",
    "        \n",
    "        batch_size = probs.shape[0]\n",
    "        \n",
    "        # Gradient of cross entropy w.r.t. softmax output\n",
    "        dprobs = probs.copy()\n",
    "        # Subtract 1 from the correct class probability\n",
    "        for i in range(batch_size):\n",
    "            dprobs[i, labels[i]] -= 1\n",
    "        dprobs /= batch_size  # Average over batch\n",
    "        \n",
    "        # Gradient w.r.t. weights and biases\n",
    "        dW_cls = cls_output.T @ dprobs\n",
    "        db_cls = np.sum(dprobs, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t. input (cls_output)\n",
    "        dcls_output = dprobs @ self.W_cls.T\n",
    "        \n",
    "        return dW_cls, db_cls, dcls_output\n",
    "\n",
    "\n",
    "class BERTSentimentClassifier:\n",
    "    def __init__(self, max_len=10, embed_dim=4, num_heads=4):\n",
    "        self.max_len = max_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_processor = VocabProcessor()\n",
    "        self.embedding_layer = None  # Will be initialized after vocabulary is built\n",
    "        self.positional_encoder = PositionalEncoder(max_len, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads)\n",
    "        self.classifier = ClassificationHead(embed_dim)\n",
    "    \n",
    "    def initialize(self, sentences):\n",
    "        # Build vocabulary\n",
    "        self.vocab_processor.build_vocab(sentences)\n",
    "        \n",
    "        # Initialize embedding layer with the vocabulary size\n",
    "        vocab_size = len(self.vocab_processor.vocab) + 1  # +1 for handling unknown tokens\n",
    "        self.embedding_layer = EmbeddingLayer(vocab_size, self.embed_dim)\n",
    "    \n",
    "    def _prepare_batch(self, sentences):\n",
    "        # Convert sentences to token IDs\n",
    "        input_ids = np.array([self.vocab_processor.tokenize(sentence) for sentence, _ in sentences])\n",
    "        labels = np.array([label for _, label in sentences])\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Embedding layer\n",
    "        embeddings = self.embedding_layer.forward(input_ids)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pos_embeddings = self.positional_encoder.forward(embeddings)\n",
    "        \n",
    "        # Transformer block\n",
    "        transformer_output = self.transformer.forward(pos_embeddings)\n",
    "        \n",
    "        # Classification logits\n",
    "        logits = self.classifier.forward(transformer_output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def train_step(self, batch, learning_rate=0.01):\n",
    "        # This is a simplified training step - in a real implementation, \n",
    "        # you would compute gradients and update weights\n",
    "        input_ids, labels = self._prepare_batch(batch)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.classifier.cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Here you would add backpropagation and parameter updates\n",
    "        # For demonstration purposes, we're returning the loss\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def predict(self, sentences):\n",
    "        # For single sentence or list of sentence strings\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [(sentences, 0)]  # Dummy label\n",
    "        elif isinstance(sentences[0], str):\n",
    "            sentences = [(sent, 0) for sent in sentences]  # Dummy labels\n",
    "            \n",
    "        input_ids, _ = self._prepare_batch(sentences)\n",
    "        logits = self.forward(input_ids)\n",
    "        return self.classifier.predict(logits)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        input_ids, true_labels = self._prepare_batch(test_data)\n",
    "        logits = self.forward(input_ids)\n",
    "        pred_labels = self.classifier.predict(logits)\n",
    "        \n",
    "        accuracy = np.mean(pred_labels == true_labels)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    sentences = [\n",
    "        (\"aku suka banget sama film ini\", 1),\n",
    "        (\"filmnya bener-bener membosankan\", 0),\n",
    "        (\"aktingnya luar biasa\", 1),\n",
    "        (\"ngantuk banget pas nonton\", 0),\n",
    "        (\"ceritanya bikin terharu\", 1),\n",
    "        (\"gak masuk akal dan jelek\", 0),\n",
    "    ]\n",
    "    \n",
    "    # Create and initialize the model\n",
    "    model = BERTSentimentClassifier(max_len=10, embed_dim=4, num_heads=4)\n",
    "    model.initialize(sentences)\n",
    "    \n",
    "    # Training loop with proper backpropagation and parameter updates\n",
    "    print(\"Starting training...\")\n",
    "    history = model.train_step(sentences, epochs=50, learning_rate=0.05, batch_size=3)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = model.evaluate(sentences)\n",
    "    print(f\"Training accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Predict new sentences\n",
    "    new_sentences = [\n",
    "        (\"film ini sangat bagus\", 0),  # Should predict 1 (positive)\n",
    "        (\"saya kecewa dengan ceritanya\", 0)  # Should predict 0 (negative)\n",
    "    ]\n",
    "    \n",
    "    predictions = model.predict(new_sentences)\n",
    "    for i, (sentence, _) in enumerate(new_sentences):\n",
    "        sentiment = \"Positive\" if predictions[i] == 1 else \"Negative\"\n",
    "        print(f\"Sentence: '{sentence}', Predicted: {sentiment}\")\n",
    "    \n",
    "    # Example of how to trace through a single forward pass to understand the model\n",
    "    def trace_example(model, sentence=\"aku suka banget sama film ini\"):\n",
    "        print(f\"\\nTracing forward pass for: '{sentence}'\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = sentence.lower().split()\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        token_ids = model.vocab_processor.tokenize(sentence)\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        \n",
    "        # Word representations\n",
    "        token_words = [model.vocab_processor.id2w.get(id, \"[UNK]\") for id in token_ids]\n",
    "        print(f\"Token representations: {token_words}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = model.predict(sentence)\n",
    "        sentiment = \"Positive\" if pred[0] == 1 else \"Negative\"\n",
    "        print(f\"Predicted sentiment: {sentiment}\")\n",
    "    \n",
    "    # Trace an example\n",
    "    trace_example(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6935, Accuracy: 0.5000\n",
      "Epoch 100, Loss: 0.6931, Accuracy: 0.5000\n",
      "Epoch 200, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 300, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 400, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 500, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 600, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 700, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 800, Loss: 0.6931, Accuracy: 1.0000\n",
      "Epoch 900, Loss: 0.6931, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            '[PAD]' : 0,\n",
    "            '[CLS]' : 1,\n",
    "        }\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        start_idx = len(self.vocab)\n",
    "        \n",
    "        for sentence, _ in sentences:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = start_idx\n",
    "                    start_idx += 1\n",
    "        \n",
    "        # mapping\n",
    "        self.w2i = {w: i  for i, w in enumerate(self.vocab)}\n",
    "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
    "\n",
    "        return self.vocab\n",
    "\n",
    "    def tokenize(self, sentence, max_len=10):\n",
    "        tokens = sentence.lower().split()\n",
    "        ids = [self.vocab[\"[CLS]\"]] + [self.vocab.get(tok, 0) for tok in tokens]  # Fixed: Should use CLS not PAD for first token\n",
    "\n",
    "        # padding\n",
    "        if len(ids) < max_len:\n",
    "            ids += [self.vocab[\"[PAD]\"]] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        \n",
    "        return ids\n",
    "\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        np.random.seed(42)\n",
    "        self.d_model = d_model\n",
    "        self.embedding_matrix = np.random.rand(vocab_size, d_model)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, input_ids, cache=True):\n",
    "        # Fixed: Handle input_ids properly as a batch of sequences\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_len = input_ids.shape[1]\n",
    "        embeddings = np.zeros((batch_size, seq_len, self.d_model))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                word_id = input_ids[i, j]\n",
    "                embeddings[i, j] = self.embedding_matrix[word_id]\n",
    "    \n",
    "        if cache:\n",
    "            self.cache['input_ids'] = input_ids\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def backward(self, dembeddings):\n",
    "        input_ids = self.cache['input_ids']\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # initialize gradients\n",
    "        dembedding_matrix = np.zeros_like(self.embedding_matrix)\n",
    "\n",
    "        # for each position in batch, accumulate gradients for corresponding words vectors\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                word_id = input_ids[i, j]\n",
    "                dembedding_matrix[word_id] += dembeddings[i, j]\n",
    "        \n",
    "        return dembedding_matrix\n",
    "\n",
    "class PositionalEncoder:\n",
    "    def __init__(self, max_len, d_model):\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.pe = self._create_postional_encoding()\n",
    "    \n",
    "    def _create_postional_encoding(self):\n",
    "        pos = np.arange(self.max_len)[:, np.newaxis]\n",
    "        i = np.arange(self.d_model)[np.newaxis, :]\n",
    "\n",
    "        # calculate angle rates\n",
    "        angle_rates = 1 / np.power(10000, (2*(i//2) / self.d_model))\n",
    "\n",
    "        # calculate angle radians\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        # apply sin to even indices and cos to odd idices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        return angle_rads\n",
    "    \n",
    "    def forward(self, embedding):\n",
    "        batch_size, seq_len, _ = embedding.shape\n",
    "        # Fixed: broadcast PE correctly to all items in batch\n",
    "        return embedding + self.pe[:seq_len, :][np.newaxis, :, :]\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "    \n",
    "        # weight initialization with proper scaling\n",
    "        self.wq = np.random.randn(d_model, num_heads * self.head_dim) * 0.01  # Fixed: shape should be (d_model, d_model)\n",
    "        self.wk = np.random.randn(d_model, num_heads * self.head_dim) * 0.01\n",
    "        self.wv = np.random.randn(d_model, num_heads * self.head_dim) * 0.01\n",
    "        self.wo = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x, cache=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Fixed: project to q, k, v with correct dimensions\n",
    "        q = x @ self.wq  # (batch_size, seq_len, d_model)\n",
    "        k = x @ self.wk\n",
    "        v = x @ self.wv\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # scale dot product attention\n",
    "        scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # softmax\n",
    "        scores -= np.max(scores, axis=-1, keepdims=True)\n",
    "        attn_weight = np.exp(scores)\n",
    "        attn_weight /= np.sum(attn_weight, axis=-1, keepdims=True) + 1e-9  # Added epsilon for numerical stability\n",
    "\n",
    "        # attention output\n",
    "        attn_output = np.matmul(attn_weight, v)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # concatenate heads and transpose back to original shape\n",
    "        attn_concat = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # output projection\n",
    "        attn_projected = attn_concat @ self.wo  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['q'] = q\n",
    "            self.cache['k'] = k\n",
    "            self.cache['v'] = v\n",
    "            self.cache['scores'] = scores\n",
    "            self.cache['attn_weight'] = attn_weight\n",
    "            self.cache['attn_output'] = attn_output\n",
    "            self.cache['attn_concat'] = attn_concat\n",
    "\n",
    "        return attn_projected\n",
    "\n",
    "    def backward(self, dattn_projected):\n",
    "        # get from cache\n",
    "        x = self.cache['x']\n",
    "        q = self.cache['q']\n",
    "        k = self.cache['k']\n",
    "        v = self.cache['v']\n",
    "        attn_weight = self.cache['attn_weight']\n",
    "        attn_output = self.cache['attn_output']\n",
    "        attn_concat = self.cache['attn_concat']\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Fixed: gradient wrt output projection wo\n",
    "        dwo = attn_concat.transpose(0, 2, 1) @ dattn_projected  # Correct: (d_model, d_model)\n",
    "        dwo = np.sum(dwo, axis=0)  # Sum over batch dimension\n",
    "        \n",
    "        # gradient wrt concatenated attention heads concat\n",
    "        dattn_concat = dattn_projected @ self.wo.T  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # reshape back to multihead-format\n",
    "        dattn_output = dattn_concat.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        \n",
    "        # gradient wrt v\n",
    "        dattn_weight = np.matmul(dattn_output, v.transpose(0, 1, 3, 2))\n",
    "        dv = np.matmul(attn_weight.transpose(0, 1, 3, 2), dattn_output)\n",
    "        \n",
    "        # gradient through softmax\n",
    "        dscores = attn_weight * (dattn_weight - np.sum(attn_weight * dattn_weight, axis=-1, keepdims=True))\n",
    "        \n",
    "        # gradient wrt q and k\n",
    "        dk_transpose = np.matmul(q.transpose(0, 1, 3, 2), dscores) / np.sqrt(self.head_dim)\n",
    "        dk = dk_transpose.transpose(0, 1, 3, 2)\n",
    "        dq = np.matmul(dscores, k) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Reshape gradients to original dimensions\n",
    "        dq = dq.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        dk = dk.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        dv = dv.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # gradient wrt input x\n",
    "        dx = (dq @ self.wq.T) + (dk @ self.wk.T) + (dv @ self.wv.T)\n",
    "        \n",
    "        # gradient wrt weights\n",
    "        dwq = x.transpose(0, 2, 1) @ dq\n",
    "        dwq = np.sum(dwq, axis=0)\n",
    "        dwk = x.transpose(0, 2, 1) @ dk\n",
    "        dwk = np.sum(dwk, axis=0)\n",
    "        dwv = x.transpose(0, 2, 1) @ dv\n",
    "        dwv = np.sum(dwv, axis=0)\n",
    "\n",
    "        return dwq, dwk, dwv, dwo, dx\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(d_model)  # Added scale parameter\n",
    "        self.beta = np.zeros(d_model)  # Added shift parameter\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        normalized = (x - mean) / std\n",
    "        out = self.gamma * normalized + self.beta  # Apply scale and shift\n",
    "\n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['mean'] = mean\n",
    "            self.cache['var'] = var\n",
    "            self.cache['std'] = std\n",
    "            self.cache['normalized'] = normalized\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # get from cache\n",
    "        x = self.cache['x']\n",
    "        mean = self.cache['mean']\n",
    "        std = self.cache['std']\n",
    "        normalized = self.cache['normalized']\n",
    "        \n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # gradient wrt gamma and beta\n",
    "        dgamma = np.sum(dout * normalized, axis=(0, 1))\n",
    "        dbeta = np.sum(dout, axis=(0, 1))\n",
    "        \n",
    "        # gradient wrt normalized x\n",
    "        dx_normalized = dout * self.gamma\n",
    "        \n",
    "        # gradient wrt var\n",
    "        dvar = np.sum(dx_normalized * (x - mean) * -0.5 * std**(-3), axis=-1, keepdims=True)\n",
    "        \n",
    "        # gradient wrt mean\n",
    "        dxmean1 = dx_normalized * -1.0/std\n",
    "        dxmean2 = dvar * -2.0 * np.mean(x - mean, axis=-1, keepdims=True)\n",
    "        \n",
    "        # gradient wrt x\n",
    "        dx = dx_normalized / std + dvar * 2 * (x - mean) / d_model + (dxmean1 + dxmean2) / d_model\n",
    "        \n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff=None):\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "    \n",
    "        # weights with proper initialization\n",
    "        self.w1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.w2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "        self.b2 = np.zeros(d_model)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # linear 1 - Expansion\n",
    "        f1 = x @ self.w1 + self.b1\n",
    "        # relu\n",
    "        r = np.maximum(0, f1)\n",
    "        # Linear 2 - Compression\n",
    "        f2 = r @ self.w2 + self.b2\n",
    "\n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['f1'] = f1\n",
    "            self.cache['r'] = r\n",
    "        \n",
    "        return f2\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        # get from cache\n",
    "        x = self.cache['x']\n",
    "        f1 = self.cache['f1']\n",
    "        r = self.cache['r']\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # gradient wrt w2 and b2\n",
    "        dr = doutput @ self.w2.T\n",
    "        dw2 = r.reshape(-1, r.shape[-1]).T @ doutput.reshape(-1, doutput.shape[-1])\n",
    "        db2 = np.sum(doutput, axis=(0, 1))\n",
    "\n",
    "        # gradient through relu\n",
    "        df1 = dr * (f1 > 0)\n",
    "\n",
    "        # gradient wrt w1 and b1\n",
    "        dw1 = x.reshape(-1, x.shape[-1]).T @ df1.reshape(-1, df1.shape[-1])\n",
    "        db1 = np.sum(df1, axis=(0, 1))\n",
    "\n",
    "        # gradient wrt x\n",
    "        dx = df1 @ self.w1.T\n",
    "        \n",
    "        return dw1, db1, dw2, db2, dx\n",
    "\n",
    "class BertModel:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, 4*d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        # multi-head attention\n",
    "        attn_output = self.attention.forward(x)\n",
    "\n",
    "        # add & norm (residual connection)\n",
    "        add1 = x + attn_output\n",
    "        norm1 = self.norm1.forward(add1)\n",
    "\n",
    "        # feed-forward\n",
    "        ff_output = self.ff.forward(norm1)  # Fixed: should use norm1 not x\n",
    "\n",
    "        # add & norm (residual connection)\n",
    "        add2 = norm1 + ff_output\n",
    "        norm2 = self.norm2.forward(add2)\n",
    "\n",
    "        if cache:\n",
    "            self.cache['x'] = x\n",
    "            self.cache['attn_output'] = attn_output\n",
    "            self.cache['add1'] = add1\n",
    "            self.cache['norm1'] = norm1\n",
    "            self.cache['ff_output'] = ff_output\n",
    "            self.cache['add2'] = add2\n",
    "\n",
    "        return norm2\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # get from cache\n",
    "        x = self.cache['x']\n",
    "        attn_output = self.cache['attn_output']\n",
    "        add1 = self.cache['add1']\n",
    "        norm1 = self.cache['norm1']\n",
    "        ff_output = self.cache['ff_output']\n",
    "        add2 = self.cache['add2']\n",
    "\n",
    "        # backward through norm2\n",
    "        dadd2, dgamma2, dbeta2 = self.norm2.backward(dout)\n",
    "\n",
    "        # gradient split at residual connection\n",
    "        dnorm1 = dadd2\n",
    "        dff_output = dadd2\n",
    "\n",
    "        # backward through feed-forward\n",
    "        dw1, db1, dw2, db2, dnorm1_ff = self.ff.backward(dff_output)\n",
    "\n",
    "        # add gradients from residual connection\n",
    "        dnorm1 += dnorm1_ff\n",
    "\n",
    "        # backward through norm1\n",
    "        dadd1, dgamma1, dbeta1 = self.norm1.backward(dnorm1)\n",
    "\n",
    "        # gradient split at residual connection\n",
    "        dx = dadd1\n",
    "        dattn_output = dadd1\n",
    "\n",
    "        # backward through attention\n",
    "        dwq, dwk, dwv, dwo, dx_att = self.attention.backward(dattn_output)\n",
    "\n",
    "        # add gradients from residual connection\n",
    "        dx += dx_att\n",
    "\n",
    "        return {\n",
    "            'dwq': dwq,\n",
    "            'dwk': dwk,\n",
    "            'dwv': dwv,\n",
    "            'dwo': dwo,\n",
    "            'dw1': dw1,\n",
    "            'db1': db1,\n",
    "            'dw2': dw2,\n",
    "            'db2': db2,\n",
    "            'dgamma1': dgamma1,\n",
    "            'dbeta1': dbeta1,\n",
    "            'dgamma2': dgamma2,\n",
    "            'dbeta2': dbeta2,\n",
    "            'dx': dx,\n",
    "        }\n",
    "\n",
    "class ClassifierHead:\n",
    "    def __init__(self, d_model, num_classes=2):\n",
    "        self.w_cls = np.random.randn(d_model, num_classes) * 0.01\n",
    "        self.b_cls = np.zeros(num_classes)\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, cache=True):\n",
    "        # take cls per batch\n",
    "        cls_output = x[:, 0, :]\n",
    "\n",
    "        # logits of that cls\n",
    "        logits = cls_output @ self.w_cls + self.b_cls\n",
    "\n",
    "        if cache:\n",
    "            self.cache['cls_output'] = cls_output\n",
    "            self.cache['logits'] = logits\n",
    "            self.cache['x'] = x\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x, axis=-1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-9)  # Added epsilon for numerical stability\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        softmax = self.softmax(logits)\n",
    "        return np.argmax(softmax, axis=-1)\n",
    "    \n",
    "    def binary_cross_entropy(self, logits, labels):\n",
    "        batch_size = logits.shape[0]\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        # store for backward propagation\n",
    "        self.cache['probs'] = probs\n",
    "        self.cache['labels'] = labels\n",
    "\n",
    "        # one-hot encode labels\n",
    "        y_one_hot = np.zeros_like(probs)\n",
    "        for i in range(batch_size):\n",
    "            y_one_hot[i, labels[i]] = 1\n",
    "            \n",
    "        # cross entropy loss\n",
    "        loss = -np.sum(y_one_hot * np.log(probs + 1e-9)) / batch_size  # Use sum for entire batch\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        # get from cache\n",
    "        probs = self.cache['probs']\n",
    "        labels = self.cache['labels']\n",
    "        cls_output = self.cache['cls_output']\n",
    "        x = self.cache['x']\n",
    "        \n",
    "        batch_size = probs.shape[0]\n",
    "\n",
    "        # gradient of cross entropy wrt softmax output (probs)\n",
    "        dprobs = probs.copy()\n",
    "        for i in range(batch_size):\n",
    "            dprobs[i, labels[i]] -= 1\n",
    "        dprobs /= batch_size\n",
    "\n",
    "        # gradient wrt weights and biases\n",
    "        dw_cls = cls_output.T @ dprobs\n",
    "        db_cls = np.sum(dprobs, axis=0)\n",
    "\n",
    "        # gradient wrt cls output\n",
    "        dcls_output = dprobs @ self.w_cls.T\n",
    "        \n",
    "        # gradient wrt full bert output (only CLS token position gets gradient)\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[:, 0, :] = dcls_output\n",
    "\n",
    "        return dw_cls, db_cls, dx\n",
    "\n",
    "class BERTSentimentClassifier:\n",
    "    def __init__(self, max_len=10, d_model=16, num_heads=4):\n",
    "        # parameters\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # vocab\n",
    "        self.vocab = Vocab()\n",
    "        # embedding\n",
    "        self.embedding = None\n",
    "        # positional encoding\n",
    "        self.pos_encoding = PositionalEncoder(max_len, d_model)\n",
    "        # bert model\n",
    "        self.model = BertModel(d_model, num_heads)\n",
    "        # classifier head\n",
    "        self.classifier = ClassifierHead(d_model)\n",
    "    \n",
    "    def initialize(self, sentences):\n",
    "        # build vocab\n",
    "        self.vocab.build_vocab(sentences)\n",
    "        # initialize embedding with vocab size\n",
    "        vocab_size = len(self.vocab.vocab) + 1\n",
    "        self.embedding = EmbeddingLayer(vocab_size, self.d_model)\n",
    "    \n",
    "    def prepare_input(self, sentences):\n",
    "        # convert sentences to tokens to indices\n",
    "        input_ids = [self.vocab.tokenize(sentence[0], self.max_len) for sentence in sentences]\n",
    "        input_ids = np.array(input_ids)\n",
    "        labels = [label for _, label in sentences]\n",
    "        labels = np.array(labels)\n",
    "        return input_ids, labels\n",
    "    \n",
    "    def forward(self, input_ids, cache=True):\n",
    "        # Embedding Layer\n",
    "        embeddings = self.embedding.forward(input_ids, cache)\n",
    "        # Positional Encoding\n",
    "        pe = self.pos_encoding.forward(embeddings)\n",
    "        # Bert Model\n",
    "        bert_output = self.model.forward(pe, cache)\n",
    "        # Classifier Head\n",
    "        logits = self.classifier.forward(bert_output, cache)\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, logits, labels):\n",
    "        # loss \n",
    "        loss = self.classifier.binary_cross_entropy(logits, labels)\n",
    "\n",
    "        # backprop through classifier\n",
    "        dw_cls, db_cls, dbert_output = self.classifier.backward()\n",
    "\n",
    "        # backprop through bert model\n",
    "        dbert_grads = self.model.backward(dbert_output)\n",
    "\n",
    "        # backprop through embedding layer\n",
    "        dembedding_matrix = self.embedding.backward(dbert_grads['dx'])\n",
    "\n",
    "        # collect all gradients for return\n",
    "        grads = {\n",
    "            'dw_cls': dw_cls,\n",
    "            'db_cls': db_cls,\n",
    "            'dembedding_matrix': dembedding_matrix,\n",
    "            'dwq': dbert_grads['dwq'],\n",
    "            'dwk': dbert_grads['dwk'],\n",
    "            'dwv': dbert_grads['dwv'],\n",
    "            'dwo': dbert_grads['dwo'],\n",
    "            'dw1': dbert_grads['dw1'],\n",
    "            'db1': dbert_grads['db1'],\n",
    "            'dw2': dbert_grads['dw2'],\n",
    "            'db2': dbert_grads['db2'],\n",
    "            'dgamma1': dbert_grads['dgamma1'],\n",
    "            'dbeta1': dbert_grads['dbeta1'],\n",
    "            'dgamma2': dbert_grads['dgamma2'],\n",
    "            'dbeta2': dbert_grads['dbeta2']\n",
    "        }\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def update_parameters(self, grads, lr=0.01):\n",
    "        # update classifier weight parameter\n",
    "        self.classifier.w_cls -= lr * grads['dw_cls']\n",
    "        self.classifier.b_cls -= lr * grads['db_cls']\n",
    "\n",
    "        # update embedding weight parameter\n",
    "        self.embedding.embedding_matrix -= lr * grads['dembedding_matrix']\n",
    "\n",
    "        # update attention weight parameter\n",
    "        self.model.attention.wq -= lr * grads['dwq']\n",
    "        self.model.attention.wk -= lr * grads['dwk']\n",
    "        self.model.attention.wv -= lr * grads['dwv']\n",
    "        self.model.attention.wo -= lr * grads['dwo']\n",
    "\n",
    "        # update layer norm parameters\n",
    "        self.model.norm1.gamma -= lr * grads['dgamma1']\n",
    "        self.model.norm1.beta -= lr * grads['dbeta1']\n",
    "        self.model.norm2.gamma -= lr * grads['dgamma2']\n",
    "        self.model.norm2.beta -= lr * grads['dbeta2']\n",
    "\n",
    "        # update ffn weight parameter\n",
    "        self.model.ff.w1 -= lr * grads['dw1']\n",
    "        self.model.ff.b1 -= lr * grads['db1']\n",
    "        self.model.ff.w2 -= lr * grads['dw2']\n",
    "        self.model.ff.b2 -= lr * grads['db2']\n",
    "\n",
    "    def train_step(self, batch, lr=0.01):\n",
    "        # prepare batch data\n",
    "        input_ids, labels = self.prepare_input(batch)\n",
    "\n",
    "        # forward pass\n",
    "        logits = self.forward(input_ids)\n",
    "\n",
    "        # backward pass\n",
    "        loss, grads = self.backward(logits, labels)\n",
    "\n",
    "        # update parameters\n",
    "        self.update_parameters(grads, lr)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def evaluate(self, sentences):\n",
    "        input_ids, labels = self.prepare_input(sentences)\n",
    "        logits = self.forward(input_ids, cache=False)\n",
    "        predictions = np.argmax(self.classifier.softmax(logits), axis=1)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy\n",
    "\n",
    "# Example\n",
    "if __name__ == \"__main__\":\n",
    "    sentences = [\n",
    "        ('aku suka film ini', 1),\n",
    "        ('aku tidak suka film ini', 0),\n",
    "        ('film ini bagus sekali', 1),\n",
    "        ('film ini sangat buruk', 0),\n",
    "    ]\n",
    "\n",
    "    # Create and initialize model\n",
    "    model = BERTSentimentClassifier(max_len=10, d_model=16, num_heads=4)\n",
    "    model.initialize(sentences)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss = model.train_step(sentences, lr=learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            accuracy = model.evaluate(sentences)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [[1,2],[2,2]]\n",
    "\n",
    "# feed forward\n",
    "\n",
    "w1 = np.random.rand(2,4)\n",
    "b1 = np.zeros(4)\n",
    "w2 = np.random.rand(4,2)\n",
    "b2 = np.zeros(2)\n",
    "\n",
    "f1 = x @ w1 + b1\n",
    "\n",
    "r = np.maximum(0, f1)\n",
    "\n",
    "f2 = r @ w2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.97258695, 3.64779082],\n",
       "       [3.06845175, 5.19621885]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
