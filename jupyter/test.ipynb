{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_for_nsp(corpus):\n",
    "    \"\"\"\n",
    "    Prepare data for NSP task by splitting corpus into sentences.\n",
    "    \"\"\"\n",
    "    # Split corpus into sentences\n",
    "    sentences = [s.strip() for s in corpus.split('.') if s.strip()]\n",
    "    \n",
    "    # Add special tokens to vocabulary\n",
    "    all_words = ' '.join(sentences).split()\n",
    "    vocab = sorted(list(set(all_words)))\n",
    "    vocab.extend(['[CLS]', '[SEP]', '[MASK]'])\n",
    "    \n",
    "    return sentences, vocab\n",
    "\n",
    "def create_sentence_pairs(sentences, positive_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Create sentence pairs for NSP task.\n",
    "    - positive_pairs: consecutive sentences (IsNext label=1)\n",
    "    - negative_pairs: random sentences (NotNext label=0)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create positive pairs (consecutive sentences)\n",
    "    for i in range(len(sentences) - 1):\n",
    "        pairs.append((sentences[i], sentences[i + 1]))\n",
    "        labels.append(1)  # IsNext\n",
    "    \n",
    "    # Create negative pairs (random sentences)\n",
    "    num_positive = len(pairs)\n",
    "    num_negative = int(num_positive * (1 - positive_ratio) / positive_ratio)\n",
    "    \n",
    "    for _ in range(num_negative):\n",
    "        idx1 = np.random.randint(0, len(sentences))\n",
    "        idx2 = np.random.randint(0, len(sentences))\n",
    "        # Make sure sentences are not consecutive\n",
    "        while abs(idx1 - idx2) == 1:\n",
    "            idx2 = np.random.randint(0, len(sentences))\n",
    "        \n",
    "        pairs.append((sentences[idx1], sentences[idx2]))\n",
    "        labels.append(0)  # NotNext\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "def tokenizer(vocab):\n",
    "    \"\"\"\n",
    "    Create token-to-index and index-to-token mappings.\n",
    "    \"\"\"\n",
    "    token2idx = {}\n",
    "    idx2token = {}\n",
    "    for idx, token in enumerate(vocab):\n",
    "        token2idx[token] = idx\n",
    "        idx2token[idx] = token\n",
    "    return token2idx, idx2token\n",
    "\n",
    "def prepare_nsp_input(sentence_pair, token2idx, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Prepare input for NSP task:\n",
    "    [CLS] sentence1 [SEP] sentence2 [SEP]\n",
    "    \"\"\"\n",
    "    tokens = ['[CLS]']\n",
    "    \n",
    "    # Add first sentence\n",
    "    tokens.extend(sentence_pair[0].split())\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    # Add second sentence\n",
    "    tokens.extend(sentence_pair[1].split())\n",
    "    tokens.append('[SEP]')\n",
    "    \n",
    "    # Convert to token ids\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        if token in token2idx:\n",
    "            token_ids.append(token2idx[token])\n",
    "        else:\n",
    "            # Handle OOV (Out of Vocabulary)\n",
    "            token_ids.append(token2idx['[MASK]'])  # Use [MASK] as OOV token\n",
    "    \n",
    "    # Create segment ids (0 for first sentence, 1 for second sentence)\n",
    "    segment_ids = []\n",
    "    segment = 0\n",
    "    for token in tokens:\n",
    "        segment_ids.append(segment)\n",
    "        if token == '[SEP]':\n",
    "            segment = 1\n",
    "    \n",
    "    # Pad or truncate to max_seq_len\n",
    "    seq_len = len(token_ids)\n",
    "    if seq_len > max_seq_len:\n",
    "        token_ids = token_ids[:max_seq_len]\n",
    "        segment_ids = segment_ids[:max_seq_len]\n",
    "    else:\n",
    "        token_ids.extend([0] * (max_seq_len - seq_len))\n",
    "        segment_ids.extend([0] * (max_seq_len - seq_len))\n",
    "    \n",
    "    return token_ids, segment_ids, min(seq_len, max_seq_len)\n",
    "\n",
    "def embedding(scale, d_model, vocab_size):\n",
    "    \"\"\"\n",
    "    Create token embedding matrix.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    embed_matrix = np.random.rand(vocab_size, d_model) * scale\n",
    "    return embed_matrix\n",
    "\n",
    "def segment_embedding(scale, d_model, num_segments=2):\n",
    "    \"\"\"\n",
    "    Create segment embedding matrix.\n",
    "    \"\"\"\n",
    "    np.random.seed(43)\n",
    "    segment_embed_matrix = np.random.rand(num_segments, d_model) * scale\n",
    "    return segment_embed_matrix\n",
    "\n",
    "def position_embedding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create position embeddings using sine and cosine functions.\n",
    "    \"\"\"\n",
    "    pos = np.arange(max_seq_len).reshape(max_seq_len, 1)\n",
    "    i = np.arange(d_model)\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros((max_seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return pe\n",
    "\n",
    "def embedding_output(token_ids, segment_ids, token_embed_matrix, segment_embed_matrix, pos_embed_matrix, seq_len):\n",
    "    \"\"\"\n",
    "    Combine token, segment, and position embeddings.\n",
    "    \"\"\"\n",
    "    token_embeds = token_embed_matrix[token_ids[:seq_len]]\n",
    "    segment_embeds = segment_embed_matrix[segment_ids[:seq_len]]\n",
    "    position_embeds = pos_embed_matrix[:seq_len]\n",
    "    \n",
    "    return token_embeds + segment_embeds + position_embeds\n",
    "\n",
    "def attention_weights(d_model):\n",
    "    \"\"\"\n",
    "    Initialize attention weights.\n",
    "    \"\"\"\n",
    "    Wq = np.random.randn(d_model, d_model) * 0.01\n",
    "    Wk = np.random.randn(d_model, d_model) * 0.01\n",
    "    Wv = np.random.randn(d_model, d_model) * 0.01\n",
    "\n",
    "    return Wq, Wk, Wv\n",
    "\n",
    "def attention_output(x, Wq, Wk, Wv, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate self-attention output.\n",
    "    \"\"\"\n",
    "    Q = x @ Wq\n",
    "    K = x @ Wk\n",
    "    V = x @ Wv\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Attention scores\n",
    "    scaled = np.matmul(Q, K.transpose()) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scaled = np.where(mask == 0, -1e9, scaled)\n",
    "    \n",
    "    # Softmax\n",
    "    softmax = np.exp(scaled) / np.sum(np.exp(scaled), axis=-1, keepdims=True)\n",
    "\n",
    "    return np.matmul(softmax, V)\n",
    "\n",
    "def add_and_norm(x, attention_output):\n",
    "    \"\"\"\n",
    "    Add and normalize (LayerNorm).\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    avg = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    norm = (x - avg) / (std + eps)\n",
    "    norm_output = attention_output + norm\n",
    "    return norm_output\n",
    "    \n",
    "def ffn_weights(d_model, d_ff):\n",
    "    \"\"\"\n",
    "    Initialize feed-forward network weights.\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(d_model, d_ff) * 0.01\n",
    "    W2 = np.random.randn(d_ff, d_model) * 0.01\n",
    "    return W1, W2\n",
    "\n",
    "def ffn_output(norm_output, W1, W2):\n",
    "    \"\"\"\n",
    "    Feed-forward network output.\n",
    "    \"\"\"\n",
    "    ffn_output = np.matmul(norm_output, W1)\n",
    "    relu = np.maximum(0, ffn_output)\n",
    "    ffn_output = np.matmul(relu, W2)\n",
    "    return ffn_output\n",
    "\n",
    "def nsp_classifier_weights(d_model):\n",
    "    \"\"\"\n",
    "    Initialize NSP classifier weights.\n",
    "    \"\"\"\n",
    "    W_nsp = np.random.randn(d_model, 2) * 0.01  # 2 classes: IsNext and NotNext\n",
    "    b_nsp = np.zeros(2)\n",
    "    return W_nsp, b_nsp\n",
    "\n",
    "def initialize_model(d_model, d_ff, vocab_size, max_seq_len):\n",
    "    \"\"\"\n",
    "    Initialize all model parameters.\n",
    "    \"\"\"\n",
    "    Wq, Wk, Wv = attention_weights(d_model)\n",
    "    W1, W2 = ffn_weights(d_model, d_ff)\n",
    "    token_embed_matrix = embedding(0.01, d_model, vocab_size)\n",
    "    segment_embed_matrix = segment_embedding(0.01, d_model)\n",
    "    pos_embed_matrix = position_embedding(max_seq_len, d_model)\n",
    "    W_nsp, b_nsp = nsp_classifier_weights(d_model)\n",
    "    \n",
    "    return {\n",
    "        'Wq': Wq, 'Wk': Wk, 'Wv': Wv,\n",
    "        'W1': W1, 'W2': W2,\n",
    "        'token_embed_matrix': token_embed_matrix,\n",
    "        'segment_embed_matrix': segment_embed_matrix,\n",
    "        'pos_embed_matrix': pos_embed_matrix,\n",
    "        'W_nsp': W_nsp, 'b_nsp': b_nsp\n",
    "    }\n",
    "\n",
    "def forward_pass(token_ids, segment_ids, seq_len, model_params, max_seq_len):\n",
    "    \"\"\"\n",
    "    Forward pass through the model for NSP task.\n",
    "    \"\"\"\n",
    "    # Embedding layer\n",
    "    embed_output = embedding_output(\n",
    "        token_ids, segment_ids,\n",
    "        model_params['token_embed_matrix'],\n",
    "        model_params['segment_embed_matrix'],\n",
    "        model_params['pos_embed_matrix'],\n",
    "        seq_len\n",
    "    )\n",
    "    \n",
    "    # Create padding mask to prevent attention to padding tokens\n",
    "    padding_mask = np.ones((seq_len, seq_len))\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if i >= seq_len or j >= seq_len:\n",
    "                padding_mask[i, j] = 0\n",
    "    \n",
    "    # Self-attention layer\n",
    "    attn_output = attention_output(\n",
    "        embed_output, model_params['Wq'], model_params['Wk'], model_params['Wv'],\n",
    "        mask=padding_mask\n",
    "    )\n",
    "    norm_output = add_and_norm(embed_output, attn_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_out = ffn_output(norm_output, model_params['W1'], model_params['W2'])\n",
    "    ffn_norm_output = add_and_norm(norm_output, ffn_out)\n",
    "    \n",
    "    # NSP classifier - use [CLS] representation\n",
    "    cls_output = ffn_norm_output[0]  # [CLS] token is the first token\n",
    "    nsp_logits = np.matmul(cls_output, model_params['W_nsp']) + model_params['b_nsp']\n",
    "    nsp_probs = np.exp(nsp_logits) / np.sum(np.exp(nsp_logits))\n",
    "    \n",
    "    return nsp_probs, cls_output\n",
    "\n",
    "def nsp_loss(probs, label):\n",
    "    \"\"\"\n",
    "    Calculate NSP loss.\n",
    "    \"\"\"\n",
    "    return -np.log(probs[label] + 1e-10)\n",
    "\n",
    "def backward_propagation(loss, probs, label, cls_output, model_params, lr=0.01):\n",
    "    \"\"\"\n",
    "    Backward propagation for NSP task.\n",
    "    \"\"\"\n",
    "    # Gradients for NSP classifier\n",
    "    d_W_nsp = np.zeros_like(model_params['W_nsp'])\n",
    "    d_b_nsp = np.zeros_like(model_params['b_nsp'])\n",
    "    \n",
    "    d_probs = np.zeros_like(probs)\n",
    "    d_probs[label] = -1.0 / (probs[label] + 1e-10)\n",
    "    \n",
    "    d_logits = probs * d_probs\n",
    "    \n",
    "    d_W_nsp += np.outer(cls_output, d_logits)\n",
    "    d_b_nsp += d_logits\n",
    "    \n",
    "    # Update parameters\n",
    "    model_params['W_nsp'] -= lr * d_W_nsp\n",
    "    model_params['b_nsp'] -= lr * d_b_nsp\n",
    "    \n",
    "    return model_params\n",
    "\n",
    "def train_nsp(corpus, n_epoch=1000, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Train the model on the NSP task.\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    d_model = 16\n",
    "    d_ff = 64\n",
    "    \n",
    "    # Prepare data\n",
    "    sentences, vocab = data_for_nsp(corpus)\n",
    "    vocab_size = len(vocab)\n",
    "    token2idx, idx2token = tokenizer(vocab)\n",
    "    \n",
    "    # Create sentence pairs and labels\n",
    "    pairs, labels = create_sentence_pairs(sentences)\n",
    "    \n",
    "    # Initialize model\n",
    "    model_params = initialize_model(d_model, d_ff, vocab_size, max_seq_len)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(pairs)):\n",
    "            # Prepare input\n",
    "            token_ids, segment_ids, seq_len = prepare_nsp_input(pairs[i], token2idx, max_seq_len)\n",
    "            label = labels[i]\n",
    "            \n",
    "            # Forward pass\n",
    "            nsp_probs, cls_output = forward_pass(token_ids, segment_ids, seq_len, model_params, max_seq_len)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss_value = nsp_loss(nsp_probs, label)\n",
    "            epoch_loss += loss_value\n",
    "            \n",
    "            # Backward propagation\n",
    "            model_params = backward_propagation(loss_value, nsp_probs, label, cls_output, model_params, lr=0.01)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = np.argmax(nsp_probs)\n",
    "            if predicted == label:\n",
    "                correct += 1\n",
    "        \n",
    "        # Average loss and accuracy for the epoch\n",
    "        avg_loss = epoch_loss / len(pairs)\n",
    "        accuracy = correct / len(pairs)\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epoch}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on a few examples\n",
    "    print(\"\\nEvaluation on sample pairs:\")\n",
    "    for i in range(min(5, len(pairs))):\n",
    "        sent1, sent2 = pairs[i]\n",
    "        token_ids, segment_ids, seq_len = prepare_nsp_input(pairs[i], token2idx, max_seq_len)\n",
    "        nsp_probs, _ = forward_pass(token_ids, segment_ids, seq_len, model_params, max_seq_len)\n",
    "        predicted = np.argmax(nsp_probs)\n",
    "        \n",
    "        print(f\"Sentence 1: {sent1}\")\n",
    "        print(f\"Sentence 2: {sent2}\")\n",
    "        print(f\"Prediction: {'IsNext' if predicted == 1 else 'NotNext'}\")\n",
    "        print(f\"Ground truth: {'IsNext' if labels[i] == 1 else 'NotNext'}\")\n",
    "        print(f\"Confidence: {nsp_probs[predicted]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return model_params, losses, accuracies\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = \"\"\"The dog went for a walk. It was a sunny day. The birds were singing in the trees. \n",
    "    A cat watched from a window. The flowers were blooming in the garden. \n",
    "    Children played in the park. A squirrel ran up a tree. The ice cream truck arrived.\"\"\"\n",
    "    \n",
    "    model_params, losses, accuracies = train_nsp(corpus, n_epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokens = set()\n",
    "for s1, s2, _ in dataset:\n",
    "    tokens.update(s1.lower().split())\n",
    "    tokens.update(s2.lower().split())\n",
    "\n",
    "tokens = sorted(list(tokens))\n",
    "vocab = {w: i+2 for i, w in enumerate(tokens)}  # 0 = PAD, 1 = CLS\n",
    "vocab[\"[PAD]\"] = 0\n",
    "vocab[\"[CLS]\"] = 1\n",
    "vocab[\"[SEP]\"] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "s1 = \"Aku pergi ke toko\".lower().split()\n",
    "s2 = \"Lalu aku membeli roti\".lower().split()\n",
    "\n",
    "input_ids = [vocab[\"[CLS]\"]] + [vocab[w] for w in s1] + [vocab[\"[SEP]\"]] + [vocab[w] for w in s2] + [vocab[\"[SEP]\"]]\n",
    "max_len = 16\n",
    "\n",
    "if len(input_ids) < max_len:\n",
    "    input_ids += [vocab[\"[PAD]\"]] * (max_len - len(input_ids))\n",
    "else:\n",
    "    input_ids = input_ids[:max_len]\n",
    "\n",
    "input_ids = np.array(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Kita pakai Mini-BERT yang sebelumnya sudah kita bangun\n",
    "hidden_states = mini_bert_forward(input_ids)\n",
    "\n",
    "# Ambil output vector dari token [CLS] (posisi pertama)\n",
    "cls_vector = hidden_states[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = cls_vector.shape[0]  # misal 32 atau 64, tergantung BERT\n",
    "\n",
    "W = np.random.randn(hidden_dim, 2) * 0.01\n",
    "b = np.zeros(2)\n",
    "\n",
    "logits = cls_vector @ W + b\n",
    "\n",
    "# Softmax\n",
    "probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "pred = np.argmax(probs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
